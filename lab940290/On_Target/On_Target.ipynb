{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "77e898b1-f0eb-48f3-b586-ca727b5e3cd9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94c87779-64ec-4463-bf03-057aea799f47",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0cd70b29-eeb5-47f5-b4d0-fff817622420",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "companies = spark.read.parquet('/dbfs/linkedin_train_data')\n",
    "display(companies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d95bd29-9ff6-43bd-a1a4-2791b882ad4d",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "filterBlob": "{\"filterGroups\":[{\"enabled\":true,\"filterGroupId\":\"fg_f5cd9ea2\",\"op\":\"OR\",\"filters\":[{\"filterId\":\"f_e63570a9\",\"enabled\":true,\"columnId\":\"id\",\"dataType\":\"string\",\"filterType\":\"oneof\",\"filterValues\":[]}],\"local\":false,\"updatedAt\":1737219907648}],\"syncTimestamp\":1737219907651}",
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "profiles = spark.read.parquet('/dbfs/linkedin_people_train_data')\n",
    "#profiles_og = spark.read.parquet('/dbfs/linkedin_people_train_data')\n",
    "display(profiles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "41d1ac53-ecc9-45bf-a54b-e373d243e9f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# PROFILES PART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "394b3b58-99d1-4d70-891d-323ab23fd3f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "'''--------------------------------------------------------------------------------------------\n",
    "                                        PROFILES\n",
    "------------------------------------------------------------------------------------------------'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f7e93008-3b84-4eeb-b1fb-ab03cff8f2a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "'''--------------------------------------------------------------------------------------------\n",
    "                                SMALL SAMPLE FOR TESTING\n",
    "------------------------------------------------------------------------------------------------'''\n",
    "\n",
    "'''\n",
    "from pyspark.sql.functions import col, size\n",
    "\n",
    "# Filter the DataFrame\n",
    "profiles = (\n",
    "    profiles\n",
    "    .filter((col(\"about\").isNotNull()) & (size(col(\"recommendations\")) > 0))\n",
    "    .limit(20)  # Keep only 20 rows\n",
    ")\n",
    "\n",
    "# Display the result\n",
    "display(profiles)'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5b938979-57e5-4b9c-842e-aa0036354ddd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# top_university "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8efeeebb-945a-4a97-b730-2d667f40b79c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "''' --------------------------------------------------------------------------------------------\n",
    "                        TOP UNIVERSITY FEATURE IN PROFILES DATAFRAME\n",
    "    -------------------------------------------------------------------------------------------- '''\n",
    "\n",
    "from pyspark.sql.functions import col, when, lower\n",
    "\n",
    "# List of US top 20 universities\n",
    "universities_names = [\n",
    "    \"princeton\", \"stanford\", \"massachusetts institute of technology\", \"yale\",\n",
    "    \"berkeley\", \"columbia\", \"university of pennsylvania\", \"harvard\", \"rice\",\n",
    "    \"cornell\", \"northwestern\", \"johns hopkins\", \"university of california\",\n",
    "    \"university of chicago\", \"vanderbilt\", \"dartmouth\", \"williams\", \"brown\",\n",
    "    \"claremont mckenna\", \"duke\"\n",
    "]\n",
    "\n",
    "# Create a regex pattern for matching any of the university names (case insensitive)\n",
    "regex_pattern = \"|\".join([fr\"\\b{university.lower()}\\b\" for university in universities_names])\n",
    "\n",
    "# Add a boolean feature indicating whether 'educations_details' matches the pattern\n",
    "profiles = profiles.withColumn(\n",
    "    \"top_university\",\n",
    "    when(lower(col(\"educations_details\")).rlike(regex_pattern), 1).otherwise(0)\n",
    ")\n",
    "\n",
    "display(profiles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eb0bca2c-b396-4fc8-848a-53d1dea82227",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# degrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b13bd747-e322-4d84-adbe-35755cb73adf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "''' --------------------------------------------------------------------------------------------\n",
    "                        DEGREES FEATURE IN PROFILES DATAFRAME \n",
    "    -------------------------------------------------------------------------------------------- '''\n",
    "    \n",
    "from pyspark.sql.functions import col, explode, collect_list, array_distinct, lower\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# 1) Define the standardization function\n",
    "def extract_standard_degree(deg_str: str) -> str:\n",
    "    if deg_str is None:\n",
    "        return None\n",
    "\n",
    "    deg_str = deg_str.lower()\n",
    "\n",
    "    # Bachelor's Degree Variations\n",
    "    bachelor_variations = [\"bachelor\", \"bsc\", \"b.a.\", \"b.a\", \"b.s.\", \"b.s\", \"bachelor of science\", \"licence\"]\n",
    "    if any(bach in deg_str for bach in bachelor_variations):\n",
    "        return \"Bachelor\"\n",
    "\n",
    "    # Master's Degree Variations\n",
    "    master_variations = [\"master\", \"msc\", \"m.s.\", \"m.s\", \"m.a.\", \"m.a\", \"master of science\", \"master of arts\"]\n",
    "    if any(mast in deg_str for mast in master_variations):\n",
    "        return \"Master\"\n",
    "\n",
    "    # Doctorate/Ph.D. Variations\n",
    "    doctorate_variations = [\"phd\", \"ph.d\", \"doctorate\", \"doctoral\", \"dr.\", \"doctor\"]\n",
    "    if any(doc in deg_str for doc in doctorate_variations):\n",
    "        return \"Doctorate\"\n",
    "\n",
    "    # Associate's Degree Variations\n",
    "    associate_variations = [\"associate\", \"a.a.\", \"a.a\", \"a.s.\", \"a.s\", \"assoc.\"]\n",
    "    if any(assoc in deg_str for assoc in associate_variations):\n",
    "        return \"Associate\"\n",
    "\n",
    "    return None  # Exclude unrecognized degrees\n",
    "\n",
    "# 2) Create UDF for Spark\n",
    "extract_standard_degree_udf = udf(extract_standard_degree, StringType())\n",
    "\n",
    "# 3) Explode, apply UDF, and filter recognized degrees\n",
    "profiles = profiles.withColumn(\"education_exploded\", explode(col(\"education\")))\n",
    "\n",
    "# Apply UDF and filter for valid degrees\n",
    "profiles = profiles.withColumn(\n",
    "    \"standardized_degree\",\n",
    "    extract_standard_degree_udf(lower(col(\"education_exploded.degree\")))\n",
    ")\n",
    "\n",
    "# 4) Use Window Function to Collect Degrees Without Aggregation\n",
    "window_spec = Window.partitionBy(\"name\")\n",
    "profiles = profiles.withColumn(\n",
    "    \"degrees\",\n",
    "    array_distinct(collect_list(\"standardized_degree\").over(window_spec))\n",
    ")\n",
    "\n",
    "# Drop intermediate columns if desired\n",
    "profiles = profiles.drop(\"education_exploded\", \"standardized_degree\")\n",
    "\n",
    "profiles = profiles.dropDuplicates([\"id\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3ebac367-29db-4c90-85d2-a0c12ed74370",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# volunteering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da6b56de-6021-4a0e-ae2a-72c8fbc02ba9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "''' --------------------------------------------------------------------------------------------\n",
    "                        VOLUNTEERING FEATURE IN PROFILES DATAFRAME\n",
    "    -------------------------------------------------------------------------------------------- '''\n",
    "\n",
    "from pyspark.sql.functions import size, col, when, explode, udf, lit, create_map\n",
    "from pyspark.sql.types import MapType, StringType, ArrayType\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "\n",
    "# Add the 'volunteering' column: 1 if 'volunteer_experience' is non-empty, otherwise 0\n",
    "profiles = profiles.withColumn(\n",
    "    \"volunteering\",\n",
    "    when(size(col(\"volunteer_experience\")) > 0, 1).otherwise(0)\n",
    ")\n",
    "\n",
    "# Créer une nouvelle colonne 'cause_volunteer' avec des causes uniques\n",
    "profiles = profiles.withColumn(\n",
    "    \"cause_volunteer\",\n",
    "    F.expr(\"array_distinct(transform(volunteer_experience, x -> x['cause']))\")\n",
    ")\n",
    "\n",
    "# ADD FROM VOLUNTEERING + FROM VOLUNTEERING CAUSES\n",
    "\n",
    "# Define a mapping function for 'cause_volunteer'\n",
    "def map_volunteering_to_values(causes):\n",
    "    values_dict = {}\n",
    "    volunteering_common_values = ['Community', 'Contribution', 'Meaningful work']\n",
    "\n",
    "    for value in volunteering_common_values:\n",
    "        # Ensure the key exists in the dictionary and initialize it as a list if needed\n",
    "        if value not in values_dict:\n",
    "            values_dict[value] = [['volunteering', None],]\n",
    "\n",
    "    if not causes:  # Handle None or empty lists\n",
    "        return values_dict\n",
    "\n",
    "    for cause in causes:\n",
    "        if cause == 'Education':\n",
    "            if 'Altruism' in values_dict:\n",
    "                values_dict['Altruism'].append(['volunteering', cause])\n",
    "            else : \n",
    "                values_dict['Altruism'] = [['volunteering', cause],]\n",
    "\n",
    "        elif cause in ['Human Rights', 'Civil Rights and Social Action']:\n",
    "            if 'Equality' in values_dict:\n",
    "                values_dict['Equality'].append(['volunteering', cause])\n",
    "            else : \n",
    "                values_dict['Equality'] = [['volunteering', cause],]\n",
    "\n",
    "        elif cause == 'Arts and Culture':\n",
    "            if 'Creativity' in values_dict:\n",
    "                values_dict['Creativity'].append(['volunteering', cause])\n",
    "            else : \n",
    "                values_dict['Creativity'] = [['volunteering', cause],]\n",
    "\n",
    "        elif cause in ['Health', 'Disaster and Humanitarian Relief', 'Animal Welfare', 'Poverty Alleviation']:\n",
    "            if 'Empathy' in values_dict:\n",
    "                values_dict['Empathy'].append(['volunteering', cause])\n",
    "            else : \n",
    "                values_dict['Empathy'] = [['volunteering', cause],]\n",
    "\n",
    "        elif cause == 'Disaster and Humanitarian Relief':\n",
    "            if 'Empathy' in values_dict:\n",
    "                values_dict['Empathy'].append(['volunteering', cause])\n",
    "            else : \n",
    "                values_dict['Empathy'] = [['volunteering', cause],]\n",
    "\n",
    "        elif cause == 'Social Services':\n",
    "            if 'Equity' in values_dict:\n",
    "                values_dict['Equity'].append(['volunteering', cause])\n",
    "            else : \n",
    "                values_dict['Equity'] = [['volunteering', cause],]\n",
    "\n",
    "        elif cause == 'Animal Welfare':\n",
    "            if 'Empathy' in values_dict:\n",
    "                values_dict['Empathy'].append(['volunteering', cause])\n",
    "            else : \n",
    "                values_dict['Empathy'] = [['volunteering', cause],]\n",
    "\n",
    "        elif cause == 'Poverty Alleviation':\n",
    "            if 'Empathy' in values_dict:\n",
    "                values_dict['Empathy'].append(['volunteering', cause])\n",
    "            else : \n",
    "                values_dict['Empathy'] = [['volunteering', cause],]\n",
    "\n",
    "        elif cause == 'Environment':\n",
    "            if 'Environment' in values_dict:\n",
    "                values_dict['Environment'].append(['volunteering', cause])\n",
    "            else:\n",
    "                values_dict['Environment'] = [['volunteering', cause],]\n",
    "\n",
    "        elif cause == 'Children':\n",
    "            if 'Altruism' in values_dict:\n",
    "                values_dict['Altruism'].append(['volunteering', cause])\n",
    "            else : \n",
    "                values_dict['Altruism'] = [['volunteering', cause],]\n",
    "\n",
    "    return values_dict\n",
    "\n",
    "# Create an empty map using create_map()\n",
    "empty_map = create_map()\n",
    "\n",
    "# Register the UDF\n",
    "map_volunteering_udf = udf(map_volunteering_to_values, MapType(StringType(), ArrayType(ArrayType(StringType()))))\n",
    "\n",
    "# Add the new column with a conditional application\n",
    "profiles = profiles.withColumn(\n",
    "    'values_from_volunteering',\n",
    "    when(\n",
    "        size(col('volunteer_experience')) > 0,  # Check if the size of volunteer_experience > 0\n",
    "        map_volunteering_udf(col('cause_volunteer'))  # Apply the UDF\n",
    "    ).otherwise(lit(None).cast(MapType(StringType(), ArrayType(ArrayType(StringType())))))  # Use null for others\n",
    ")\n",
    "\n",
    "# Display the updated DataFrame\n",
    "display(profiles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0c84b1f6-b1cb-4707-9549-59e73e2a4a8b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# profile picture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b5dac5a-ce6d-4905-a5c5-db62ff543345",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "''' --------------------------------------------------------------------------------------------\n",
    "                        PROFILE PICTURE FEATURE IN PROFILES DATAFRAME\n",
    "    -------------------------------------------------------------------------------------------- '''\n",
    "    \n",
    "!pip install ultralytics \n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StringType\n",
    "from huggingface_hub import hf_hub_download\n",
    "from ultralytics import YOLO\n",
    "from transformers import pipeline\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "# Load YOLO model for face detection\n",
    "model_path = hf_hub_download(repo_id=\"arnabdhar/YOLOv8-Face-Detection\", filename=\"model.pt\")\n",
    "face_model = YOLO(model_path)\n",
    "\n",
    "# Load emotion detection pipeline\n",
    "emotion_pipe = pipeline(\"image-classification\", model=\"jayanta/microsoft-resnet-50-cartoon-emotion-detection\")\n",
    "\n",
    "# Define the processing function\n",
    "def process_profile_picture(image_url):\n",
    "    try:\n",
    "        # Download the image\n",
    "        response = requests.get(image_url)\n",
    "        if response.status_code != 200:\n",
    "            return \"0\"  # Return \"0\" if image can't be fetched\n",
    "\n",
    "        image = Image.open(BytesIO(response.content))\n",
    "\n",
    "        # Face detection\n",
    "        face_output = face_model(image)\n",
    "        if len(face_output[0].boxes) == 0:\n",
    "            return \"0\"  # No face detected\n",
    "\n",
    "        # Emotion detection\n",
    "        emotions = emotion_pipe(image)\n",
    "        return [emotions]  # Convert to string for PySpark compatibility\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing URL {image_url}: {e}\")\n",
    "        return \"0\"  # Return \"0\" for any errors\n",
    "\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import ArrayType\n",
    "\n",
    "# Define the UDF\n",
    "process_profile_picture_udf = udf(process_profile_picture, ArrayType(StringType()))\n",
    "\n",
    "# Apply the UDF to the 'avatar' column and create a new column 'avatar_emotions'\n",
    "profiles = profiles.withColumn(\"avatar_emotions\", process_profile_picture_udf(F.col(\"avatar\")))\n",
    "\n",
    "# Display the updated DataFrame\n",
    "display(profiles)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c329b8b5-8a87-4e17-837c-936fff906404",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# average post duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bdf20546-4319-4632-8e64-92c2cd144f49",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "''' --------------------------------------------------------------------------------------------\n",
    "                        AVERAGE POST DURATION FEATURE IN PROFILES DATAFRAME\n",
    "    -------------------------------------------------------------------------------------------- '''\n",
    "\n",
    "from pyspark.sql.functions import col, when, regexp_extract, explode, udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "# Define a UDF to convert \"duration_short\" to total months\n",
    "def duration_to_months(duration_short):\n",
    "    if duration_short is None:\n",
    "        return None\n",
    "    import re\n",
    "    match = re.match(r'(?:(\\d+)\\s*years?)?\\s*(?:(\\d+)\\s*months?)?', duration_short)\n",
    "    if not match:\n",
    "        return None\n",
    "    years = int(match.group(1)) if match.group(1) else 0\n",
    "    months = int(match.group(2)) if match.group(2) else 0\n",
    "    return years * 12 + months\n",
    "\n",
    "# Register the UDF\n",
    "duration_to_months_udf = F.udf(duration_to_months, returnType=IntegerType())\n",
    "\n",
    "# Explode the \"experience\" array to process each element individually\n",
    "exploded_df = profiles.withColumn(\"experience_exploded\", F.explode(\"experience\"))\n",
    "\n",
    "# Add a column for the total months for each experience\n",
    "exploded_df = exploded_df.withColumn(\n",
    "    \"months\",\n",
    "    duration_to_months_udf(F.col(\"experience_exploded.duration_short\"))\n",
    ")\n",
    "\n",
    "# Group by original row and compute the average months (ignoring nulls) and round to one decimal place\n",
    "average_months_df = exploded_df.groupBy(\"id\").agg(\n",
    "    F.round(F.avg(\"months\"), 1).alias(\"average_months_of_experience\")\n",
    ")\n",
    "\n",
    "# Join the result back to the original DataFrame\n",
    "profiles = profiles.join(average_months_df, on=\"id\", how=\"left\")\n",
    "\n",
    "display(profiles)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7f8ac536-deb1-4802-89cd-9f102682f737",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# organization Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a3f57ca-f525-46a4-9a96-20ffa00f8a01",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "''' --------------------------------------------------------------------------------------------\n",
    "                    ORGANIZATION TYPE FROM EXPERIENCE FEATURE IN PROFILES DATAFRAME --- \n",
    "    -------------------------------------------------------------------------------------------- '''\n",
    "\n",
    "\n",
    "from pyspark.sql.functions import collect_list, explode, array_distinct, array_union, coalesce, col, lit,struct, size\n",
    "\n",
    "\n",
    "# Step 1: Group profiles by company name and collect employee IDs\n",
    "companies_with_employees = (\n",
    "    profiles.groupBy(\"current_company:name\")\n",
    "    .agg(collect_list(\"id\").alias(\"employee_ids\"))\n",
    ")\n",
    "\n",
    "# Step 2: Extract experience details for each profile\n",
    "profiles_pushed = profiles.select('id', 'experience')\n",
    "exploded_profiles = profiles_pushed.withColumn(\"experience_exploded\", explode(\"experience\"))\n",
    "\n",
    "# Step 3: Extract 'id' and 'subtitle' from the exploded experience column\n",
    "profiles_with_subtitle = exploded_profiles.select(\n",
    "    \"id\", col(\"experience_exploded.subtitle\").alias(\"subtitle\")\n",
    ")\n",
    "\n",
    "# Step 4: Group by 'id' and collect all 'subtitle' values into a list\n",
    "grouped_profiles = profiles_with_subtitle.groupBy(\"id\").agg(\n",
    "    collect_list(\"subtitle\").alias(\"subtitles\")\n",
    ")\n",
    "\n",
    "# Step 5: Explode subtitles to map subtitles to IDs\n",
    "exploded_subtitles = grouped_profiles.withColumn(\"subtitle\", explode(\"subtitles\")).select('id', 'subtitle')\n",
    "\n",
    "# Step 6: Group by subtitle to collect IDs\n",
    "subtitles_with_ids = exploded_subtitles.groupBy(\"subtitle\").agg(\n",
    "    collect_list(\"id\").alias(\"ids\")\n",
    ")\n",
    "\n",
    "# Step 7: Perform a full outer join between companies_with_employees and subtitles_with_ids\n",
    "updated_companies_with_employees = companies_with_employees.join(\n",
    "    subtitles_with_ids,\n",
    "    companies_with_employees[\"current_company:name\"] == subtitles_with_ids[\"subtitle\"],\n",
    "    \"full_outer\"\n",
    ")\n",
    "\n",
    "# Step 8: Merge the 'employee_ids' and 'ids' columns, ensuring unique values\n",
    "companies_employees = updated_companies_with_employees.withColumn(\n",
    "    \"employee_ids\",\n",
    "    array_union(\n",
    "        coalesce(col(\"employee_ids\"), lit([])),  # Replace null with an empty list\n",
    "        coalesce(col(\"ids\"), lit([]))  # Replace null with an empty list\n",
    "    )\n",
    ").select(\n",
    "    coalesce(col(\"current_company:name\"), col(\"subtitle\")).alias(\"company_name\"),\n",
    "    \"employee_ids\"\n",
    ")\n",
    "\n",
    "# Step 9: Add organization_type information from the companies DataFrame\n",
    "# Only keep companies present in both DataFrames\n",
    "companies_employees_filtered = companies_employees.join(\n",
    "    companies,\n",
    "    companies_employees[\"company_name\"] == companies[\"name\"],\n",
    "    \"inner\"  # Inner join to keep only matched companies\n",
    ").select(\n",
    "    \"company_name\", \"employee_ids\", \"organization_type\"\n",
    ")\n",
    "\n",
    "# Step 10: Explode employee_ids to assign organization types to each ID\n",
    "exploded_employees = companies_employees_filtered.withColumn(\"id\", explode(\"employee_ids\"))\n",
    "\n",
    "# Step 11: Group by 'id' and collect all organization types worked in\n",
    "types_per_id = exploded_employees.groupBy(\"id\").agg(\n",
    "    array_distinct(\n",
    "        collect_list(\n",
    "            struct(col(\"company_name\"), col(\"organization_type\"))\n",
    "        )\n",
    "    ).alias(\"company_and_organization_types\")\n",
    ")\n",
    "\n",
    "profiles= profiles.join(types_per_id, on='id', how='left')\n",
    "\n",
    "\n",
    "display(profiles)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2a00a443-1a0b-4c8e-9ae2-3f4981e4f3bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# profile values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd393683-0760-4b74-a29f-faff21963e39",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "''' --------------------------------------------------------------------------------------------\n",
    "        VALUES FEATURE ON PROFILES DATAFRAME WITH SOURCES INFERED DIRECTLY FROM FEATURES\n",
    "    -------------------------------------------------------------------------------------------- '''\n",
    "    \n",
    "# ------------------------------- ADD FROM OTHER FEATURES -------------------------------\n",
    "\n",
    "profiles = profiles.withColumn(\"values_from_features\", F.struct())\n",
    "\n",
    "# ADD FROM TOP_UNIVERSITY\n",
    "top_university_features = ['Excellence', 'Learning']\n",
    "\n",
    "profiles = profiles.withColumn(\n",
    "    \"values_from_features\",\n",
    "    F.col(\"values_from_features\").withField(\n",
    "        \"top_university\",\n",
    "        F.when(\n",
    "            F.col(\"top_university\") == 1,\n",
    "            F.array(*[F.lit(x) for x in top_university_features])\n",
    "        ).otherwise(F.lit(None).cast(\"array<string>\"))  # ensure consistent type\n",
    "    )\n",
    ")\n",
    "\n",
    "# ADD FROM DEGREES\n",
    "doctorate_features = ['Excellence', 'Autonomy', 'Learning']\n",
    "\n",
    "profiles = profiles.withColumn(\n",
    "    \"values_from_features\",\n",
    "    F.col(\"values_from_features\").withField(\n",
    "        \"degrees\",\n",
    "        F.when(\n",
    "            F.array_contains(F.col(\"degrees\"), \"Doctorate\"),\n",
    "            F.array(*[F.lit(x) for x in doctorate_features])\n",
    "        ).otherwise(F.lit(None).cast(\"array<string>\"))\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# ADD FROM ORGANIZATION_TYPE\n",
    "nonprofit_features = ['Contribution', 'Meaningful work', 'Community']\n",
    "self_employed_features = ['Accountability', 'Autonomy']\n",
    "\n",
    "profiles = profiles.withColumn(\n",
    "    \"values_from_features\",\n",
    "    F.col(\"values_from_features\").withField(\n",
    "        \"company_and_organization_types\",\n",
    "        F.when(\n",
    "            F.expr(\"array_contains(transform(company_and_organization_types, x -> x['organization_type']), 'Nonprofit')\"),\n",
    "            F.array(*[F.lit(x) for x in nonprofit_features])\n",
    "        )\n",
    "        .when(\n",
    "            F.expr(\"array_contains(transform(company_and_organization_types, x -> x['organization_type']), 'Self-Employed')\"),\n",
    "            F.array(*[F.lit(x) for x in self_employed_features])\n",
    "        )\n",
    "        .otherwise(F.lit(None).cast(\"array<string>\"))\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd21f0e0-17e1-41c5-b4d4-fa6362cb5ad6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "''' --------------------------------------------------------------------------------------------\n",
    "                        EXTRACTED KEYWORDS FEATURED IN PROFILES DATAFRAME --- PART 1\n",
    "    -------------------------------------------------------------------------------------------- '''\n",
    "\n",
    "\n",
    "profiles_pushed_for_keywords = profiles.select('id', 'about', 'recommendations')\n",
    "\n",
    "\n",
    "# FROM 'ABOUT' SECTION \n",
    "\n",
    "# Extract KEYWORDS\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "import nltk\n",
    "\n",
    "# Step 1: Define UDF with NLTK Resource Download and Filters\n",
    "def extract_keywords_nltk(text):\n",
    "    if text is None :\n",
    "        return None\n",
    "    # Ensure NLTK resources are available on executors\n",
    "    nltk.download('punkt', quiet=True)\n",
    "    nltk.download('averaged_perceptron_tagger', quiet=True)\n",
    "    nltk.download('stopwords', quiet=True)\n",
    "\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    from nltk.corpus import stopwords\n",
    "    from nltk import pos_tag\n",
    "\n",
    "    # Tokenize and tag POS\n",
    "    tokens = word_tokenize(text)\n",
    "    pos_tags = pos_tag(tokens)\n",
    "\n",
    "    # Extract nouns and adjectives, lowercase, and filter for letters and hyphens\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    keywords = [\n",
    "        word.lower() for word, pos in pos_tags\n",
    "        if (pos.startswith('NN') or pos.startswith('JJ'))  # Only nouns and adjectives\n",
    "        and word.lower() not in stop_words  # Remove stopwords\n",
    "        and all(c.isalpha() or c == '-' for c in word)  # Allow only letters and hyphens\n",
    "    ]\n",
    "\n",
    "    # Return distinct keywords\n",
    "    return list(set(keywords))\n",
    "\n",
    "# Step 2: Register UDF\n",
    "nltk_udf = udf(extract_keywords_nltk, ArrayType(StringType()))\n",
    "\n",
    "# Step 4: Apply UDF to Add 'about_keywords' Column\n",
    "profiles_pushed_for_keywords = profiles_pushed_for_keywords.withColumn(\"about_keywords\", nltk_udf(profiles_pushed_for_keywords[\"about\"]))\n",
    "\n",
    "# FROM RECOMMENDATIONS SECTION \n",
    "\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "import nltk\n",
    "\n",
    "# Step 1: Define UDF for Recommendations\n",
    "def extract_keywords_from_list_nltk(recommendations):\n",
    "    if not recommendations:\n",
    "        return []\n",
    "    import nltk\n",
    "    nltk.download('punkt', quiet=True)\n",
    "    nltk.download('averaged_perceptron_tagger', quiet=True)\n",
    "    nltk.download('stopwords', quiet=True)\n",
    "\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    from nltk.corpus import stopwords\n",
    "    from nltk import pos_tag\n",
    "\n",
    "    # Ensure input is a valid list\n",
    "    if not isinstance(recommendations, list):\n",
    "        return []\n",
    "\n",
    "    # Define stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    # Initialize an empty set for unique keywords\n",
    "    keywords_set = set()\n",
    "\n",
    "    # Process each recommendation in the list\n",
    "    for text in recommendations:\n",
    "        if not isinstance(text, str) or not text.strip():  # Skip non-string or empty values\n",
    "            continue\n",
    "        try:\n",
    "            # Tokenize and tag POS\n",
    "            tokens = word_tokenize(text)\n",
    "            pos_tags = pos_tag(tokens)\n",
    "\n",
    "            # Extract nouns and adjectives, lowercase, and filter for letters and hyphens\n",
    "            keywords = [\n",
    "                word.lower() for word, pos in pos_tags\n",
    "                if (pos.startswith('NN') or pos.startswith('JJ'))  # Only nouns and adjectives\n",
    "                and word.lower() not in stop_words  # Remove stopwords\n",
    "                and all(c.isalpha() or c == '-' for c in word)  # Allow only letters and hyphens\n",
    "            ]\n",
    "\n",
    "            # Add keywords to the set\n",
    "            keywords_set.update(keywords)\n",
    "        except Exception as e:\n",
    "            # Log or handle unexpected errors for specific strings\n",
    "            print(f\"Error processing text: {text}, error: {e}\")\n",
    "            continue\n",
    "\n",
    "    # Return distinct keywords as a list\n",
    "    return list(keywords_set)\n",
    "\n",
    "# Step 2: Register UDF for Recommendations\n",
    "recommendations_udf = udf(extract_keywords_from_list_nltk, ArrayType(StringType()))\n",
    "\n",
    "# Step 3: Apply UDF to Add 'recommendations_keywords' Column\n",
    "profiles_pushed_for_keywords = profiles_pushed_for_keywords.withColumn(\"recommendations_keywords\", recommendations_udf(profiles_pushed_for_keywords[\"recommendations\"]))\n",
    "\n",
    "display(profiles_pushed_for_keywords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e41fa69-9293-48e9-b4b6-0efa12effe74",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "''' --------------------------------------------------------------------------------------------\n",
    "                        VALUES FEATURE ON PROFILES DATAFRAME ---- PART 2\n",
    "    -------------------------------------------------------------------------------------------- '''\n",
    "\n",
    "# ------------------------------- MODEL BASED - WITH CONTEXT -------------------------------\n",
    "\n",
    "'''from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "import torch\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "\n",
    "# Define the label columns and threshold\n",
    "THRESHOLD = 0.6\n",
    "LABEL_COLUMNS = ['Self-direction: thought', 'Self-direction: action', 'Stimulation', 'Hedonism', 'Achievement',\n",
    "                 'Power: dominance', 'Power: resources', 'Face', 'Security: personal', 'Security: societal',\n",
    "                 'Tradition', 'Conformity: rules', 'Conformity: interpersonal', 'Humility', 'Benevolence: caring',\n",
    "                 'Benevolence: dependability', 'Universalism: concern', 'Universalism: nature', 'Universalism: tolerance',\n",
    "                 'Universalism: objectivity']\n",
    "\n",
    "# Define a function to apply the model to a single string\n",
    "def apply_model_to_text(text):\n",
    "    if not text:\n",
    "        return None\n",
    "\n",
    "    # Load the model and tokenizer inside the function\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"tum-nlp/Deberta_Human_Value_Detector\")\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\"tum-nlp/Deberta_Human_Value_Detector\", trust_remote_code=True)\n",
    "\n",
    "    encoding = tokenizer.encode_plus(\n",
    "        text,\n",
    "        add_special_tokens=True,\n",
    "        max_length=512,\n",
    "        return_token_type_ids=False,\n",
    "        padding=\"max_length\",\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt',\n",
    "    )\n",
    "\n",
    "    with torch.no_grad():\n",
    "        prediction = model(encoding[\"input_ids\"], encoding[\"attention_mask\"])\n",
    "        prediction = prediction[\"output\"].flatten().numpy()\n",
    "\n",
    "    # Filter labels based on threshold\n",
    "    results = []\n",
    "    for label, score in zip(LABEL_COLUMNS, prediction):\n",
    "        if score >= THRESHOLD:\n",
    "            results.append(f\"{label}: {score:.4f}\")\n",
    "    return results\n",
    "\n",
    "\n",
    "# FROM ABOUT SECTION \n",
    "# Define a UDF for the function\n",
    "@udf(ArrayType(StringType()))\n",
    "def apply_model_udf(text):\n",
    "    if not text :\n",
    "        return []\n",
    "    result = apply_model_to_text(text)\n",
    "    # Extract the first word before the first colon\n",
    "    extracted_words = [item.split(':')[0] for item in result]\n",
    "    return extracted_words\n",
    "\n",
    "# Apply the UDF to the 'about' column and create a new column 'about_context'\n",
    "profiles_pushed_for_keywords = profiles_pushed_for_keywords.withColumn(\"about_context\", apply_model_udf(profiles_pushed_for_keywords[\"about\"]))\n",
    "\n",
    "\n",
    "# FROM RECOMMENDATIONS SECTION \n",
    "# Define a UDF to handle lists of recommendations\n",
    "@udf(returnType=ArrayType(StringType()))\n",
    "def process_recommendations(recommendations):\n",
    "    if not recommendations:  # Handles both None and empty list\n",
    "        return None\n",
    "    results = []\n",
    "    for rec in recommendations:\n",
    "        results.extend(apply_model_to_text(rec))\n",
    "    # Extract the first word before the first colon\n",
    "    extracted_words = [item.split(':')[0] for item in results]\n",
    "    return extracted_words\n",
    "\n",
    "# Apply the UDF to the DataFrame\n",
    "profiles_pushed_for_keywords = profiles_pushed_for_keywords.withColumn(\"recommendations_context\", process_recommendations(profiles_pushed_for_keywords[\"recommendations\"]))\n",
    "\n",
    "display(profiles_pushed_for_keywords)'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2de0f678-1618-41f3-b415-d58e7b37b275",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "''' --------------------------------------------------------------------------------------------\n",
    "                        VALUES FEATURE ON PROFILES DATAFRAME ---- PART 3\n",
    "    -------------------------------------------------------------------------------------------- '''\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from sparknlp.base import *\n",
    "from sparknlp.annotator import *\n",
    "\n",
    "def get_values_dict():\n",
    "    ''' --------------------------------------------------------------------------------------------\n",
    "                                        VALUES DICTIONARY\n",
    "    -------------------------------------------------------------------------------------------- '''\n",
    "    \n",
    "    core_values_lexical_field = {\n",
    "    \"Accountability\": [\n",
    "        \"responsibility\", \n",
    "        \"responsible\"\n",
    "        \"ownership\", \n",
    "        \"reliability\"\n",
    "    ],\n",
    "    \"Adaptability\": [\n",
    "        \"flexible\", \n",
    "        \"resilient\", \n",
    "        \"versatile\", \n",
    "        \"adaptive\"\n",
    "    ],\n",
    "    \"Achievement\": [\n",
    "        \"accomplishment\", \n",
    "        \"success\", \n",
    "        \"victory\", \n",
    "        \"performance\",\n",
    "        \"productivity\"\n",
    "    ],\n",
    "    \"Passion\": ['passion',\n",
    "        'dedication',\n",
    "        'enthousiasm',\n",
    "        'commitment'],\n",
    "    \"Empathy\": [\n",
    "        \"compassion\", \n",
    "        \"kindness\", \n",
    "        \"benevolence\", \n",
    "        \"charity\",\n",
    "        \"empathy\"\n",
    "    ],\n",
    "    \"Collaboration\": [\n",
    "        \"cooperation\", \n",
    "        \"teamwork\", \n",
    "        \"partnership\", \n",
    "        \"collaborative\",\n",
    "        \"collaboration\",\n",
    "        \"synergy\"\n",
    "    ],\n",
    "    \"Communication\": [\n",
    "        \"dialogue\", \n",
    "        \"interaction\", \n",
    "        \"exchanging\",\n",
    "        \"communicate\",\n",
    "    ],\n",
    "    \"Community\": [\n",
    "        \"society\", \n",
    "        \"collective\", \n",
    "        \"network\"\n",
    "    ],\n",
    "\n",
    "    \"Creativity\": [\n",
    "        \"originality\",\n",
    "        \"creativity\",\n",
    "        \"artist\"\n",
    "    ],\n",
    "\n",
    "    \"Altruism\":[\n",
    "        \"generosity\",\n",
    "        \"altruism\",\n",
    "        \"nonprofit\"\n",
    "    ],\n",
    "\n",
    "    \"Contribution\": [\n",
    "        \"donation\", \n",
    "        \"engagement\", \n",
    "        \"participation\", \n",
    "        \"involvement\"\n",
    "    ],\n",
    "    \"Innovation\": [\n",
    "        \"creative\", \n",
    "        \"ingenuity\", \n",
    "        \"inventive\",\n",
    "        \"innovation\",\n",
    "        \"visionary\"\n",
    "    ],\n",
    "    \"Diversity\": [\n",
    "        \"inclusion\", \n",
    "        \"heterogeneity\"\n",
    "        \"diversity\",\n",
    "        \"multiculturalism\"\n",
    "    ],\n",
    "    \"Environment\": [\n",
    "        \"sustainability\", \n",
    "        \"eco\",\n",
    "        \"environment\"\n",
    "        \"ecology\",\n",
    "        \"eco-friendly\", \n",
    "        \"long-term viability\", \n",
    "        \"conservation\", \n",
    "        \"renewability\",\n",
    "        \"green\",\n",
    "        \"sustainability\"\n",
    "    ],\n",
    "    \"Equality\": [\n",
    "        \"equity\", \n",
    "        \"fairness\", \n",
    "        \"impartiality\", \n",
    "        \"evenness\"\n",
    "    ],\n",
    "    \"Growth\": [\n",
    "        \"development\", \n",
    "        \"progress\", \n",
    "        \"advancement\", \n",
    "        \"expansion\"\n",
    "    ],\n",
    "    \"Work Ethic\": [\n",
    "        \"professionalism\",\n",
    "        \"ethic\",\n",
    "        \"diligence\"\n",
    "    ],\n",
    "    \"Integrity\": [\n",
    "        \"honesty\", \n",
    "        \"authenticity\", \n",
    "        \"fair\", \n",
    "        \"sincere\", \n",
    "        \"openness\"\n",
    "    ],\n",
    "    \"Leadership\": [\n",
    "        \"guidance\", \n",
    "        \"directive\", \n",
    "        \"influent\", \n",
    "        \"authority\"\n",
    "    ],\n",
    "    \"Learning\": [\n",
    "        \"knowledge\", \n",
    "        \"training\",\n",
    "        \"self-improvement\"\n",
    "    ],\n",
    "    \"Perseverance\": [\n",
    "        \"determination\", \n",
    "        \"tenacity\", \n",
    "        \"resolve\", \n",
    "        \"steadfastness\",\n",
    "    ],\n",
    "    \"Respect\": [\n",
    "        \"admiration\", \n",
    "        \"regard\", \n",
    "        \"deference\", \n",
    "        \"esteem\"\n",
    "    ],\n",
    "    \"Balance\": [\n",
    "        \"stability\", \n",
    "        \"equilibrium\", \n",
    "        \"wellness\", \n",
    "        \"balance\"\n",
    "    ],\n",
    "    \"High Standards\": [\n",
    "        \"quality\",\n",
    "        \"excellence\",\n",
    "        \"expertise\"\n",
    "    ],\n",
    "\n",
    "    'Meaningful work': [\n",
    "        \"meaningful\",\n",
    "    ],\n",
    "\n",
    "    \"Autonomy\": [\n",
    "        \"autonomy\",\n",
    "        \"independant\",\n",
    "        \"self driven\"\n",
    "    ]\n",
    "    }\n",
    "\n",
    "    def add_stemmed_and_lemmatized_words_to_dictionary(core_values_lexical_field):\n",
    "        \"\"\"\n",
    "        Adds stemmed and lemmatized words to the list of values for each key in the dictionary.\n",
    "        \"\"\"\n",
    "        # Convert dictionary into a DataFrame for processing\n",
    "        key_value_pairs = [(key, word) for key, values in core_values_lexical_field.items() for word in values]\n",
    "        data = spark.createDataFrame(key_value_pairs, [\"key\", \"word\"])\n",
    "    \n",
    "        # Define the Spark NLP pipeline for stemming and lemmatization\n",
    "        document_assembler = DocumentAssembler() \\\n",
    "        .setInputCol(\"word\") \\\n",
    "        .setOutputCol(\"document\")\n",
    "\n",
    "        tokenizer = Tokenizer() \\\n",
    "        .setInputCols([\"document\"]) \\\n",
    "        .setOutputCol(\"token\")\n",
    "\n",
    "        stemmer = Stemmer() \\\n",
    "        .setInputCols([\"token\"]) \\\n",
    "        .setOutputCol(\"stem\")\n",
    "\n",
    "        lemmatizer = LemmatizerModel.pretrained() \\\n",
    "        .setInputCols([\"token\"]) \\\n",
    "        .setOutputCol(\"lemma\")\n",
    "\n",
    "        pipeline = Pipeline(stages=[\n",
    "        document_assembler,\n",
    "        tokenizer,\n",
    "        stemmer,\n",
    "        lemmatizer\n",
    "        ])\n",
    "\n",
    "        # Apply the pipeline to the data\n",
    "        model = pipeline.fit(data)\n",
    "        result = model.transform(data)\n",
    "\n",
    "        # Extract original words, stems, lemmas, and keys\n",
    "        result = result.selectExpr(\"key\", \"word\", \"explode(stem.result) as stemmed\", \"explode(lemma.result) as lemmatized\")\n",
    "    \n",
    "        # Convert the DataFrame back into a dictionary with stemmed and lemmatized words added\n",
    "        result_rdd = result.rdd.map(lambda row: (row['key'], row['word'], row['stemmed'], row['lemmatized']))\n",
    "        updated_dict = {}\n",
    "\n",
    "        for key, word, stemmed, lemmatized in result_rdd.collect():\n",
    "            if key not in updated_dict:\n",
    "                updated_dict[key] = set(core_values_lexical_field[key])  # Add original words\n",
    "            updated_dict[key].add(stemmed)  # Add the stemmed word\n",
    "            updated_dict[key].add(lemmatized)  # Add the lemmatized word\n",
    "\n",
    "        # Convert sets back to sorted lists\n",
    "        for key in updated_dict:\n",
    "            updated_dict[key] = sorted(updated_dict[key])\n",
    "\n",
    "        return updated_dict\n",
    "    \n",
    "    # Add stemmed and lemmatized words to the dictionary\n",
    "    core_values_lexical_field_with_stemmed_and_lemmatized = add_stemmed_and_lemmatized_words_to_dictionary(core_values_lexical_field)\n",
    "\n",
    "    return core_values_lexical_field_with_stemmed_and_lemmatized\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4408aa9f-b7dc-4024-bbc8-3b0d1142b1c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "''' --------------------------------------------------------------------------------------------\n",
    "                        VALUES FEATURE ON PROFILES DATAFRAME ---- PART 4\n",
    "    -------------------------------------------------------------------------------------------- '''\n",
    "    \n",
    "# -------------------- GATHER ALL DETECTED KEYWORDS AND MODEL-BASED VALUES  -------------------\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "columns = ['about_context', 'recommendations_context', 'about_keywords', 'recommendations_keywords']\n",
    "\n",
    "'''# Create the `keywords_agg` column\n",
    "profiles_pushed_for_keywords = profiles_pushed_for_keywords.withColumn(\n",
    "    'keywords_agg',\n",
    "    F.array_distinct(F.flatten(F.array('about_context', 'recommendations_context', 'about_keywords', 'recommendations_keywords')))\n",
    ")'''\n",
    "\n",
    "# Create the `keywords_agg` column\n",
    "profiles_pushed_for_keywords = profiles_pushed_for_keywords.withColumn(\n",
    "    'keywords_agg',\n",
    "    (F.flatten(F.array('about_keywords', 'recommendations_keywords')))\n",
    ")\n",
    "display(profiles_pushed_for_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff753b9b-a403-4af7-8237-3011346a7241",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "''' --------------------------------------------------------------------------------------------\n",
    "                        VALUES FEATURE ON PROFILES DATAFRAME ---- PART 5 --- VERSION 1\n",
    "    -------------------------------------------------------------------------------------------- '''\n",
    "    \n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "\n",
    "# Step 1. Create and broadcast your dictionary on the driver\n",
    "dict_values = get_values_dict()\n",
    "bc_dict_values = spark.sparkContext.broadcast(dict_values)\n",
    "\n",
    "# Step 2. Define your UDF with the broadcast variable as a default parameter.\n",
    "@udf(returnType=ArrayType(StringType()))\n",
    "def filter_map_udf(keywords, broadcasted_dict=bc_dict_values):\n",
    "    import nltk\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "    # Download NLTK resources (this will run on each executor)\n",
    "    nltk.download(\"wordnet\", quiet=True)\n",
    "    nltk.download(\"omw-1.4\", quiet=True)\n",
    "    \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    def lemmatize_word(word):\n",
    "        return lemmatizer.lemmatize(word.lower(), pos=\"n\") if word else \"\"\n",
    "    \n",
    "    if not keywords:\n",
    "        return []\n",
    "    \n",
    "    # IMPORTANT: Use the parameter name, not bc_dict_values!\n",
    "    local_dict = broadcasted_dict.value\n",
    "    \n",
    "    matched_values = []\n",
    "    for kw in keywords:\n",
    "        kw_lemma = lemmatize_word(kw)\n",
    "        for core_val, synonyms_set in local_dict.items():\n",
    "            if kw_lemma in synonyms_set:\n",
    "                matched_values.append(core_val)\n",
    "                break\n",
    "    return matched_values\n",
    "\n",
    "# Step 3. Apply the UDF to your DataFrame\n",
    "profiles_pushed_for_keywords = profiles_pushed_for_keywords.withColumn(\n",
    "    \"profile_values_not_sourced\", filter_map_udf(col(\"keywords_agg\"))\n",
    ")\n",
    "\n",
    "''' --------------------------------------------------------------------------------------------\n",
    "            JOINING VALUES FEATURE ON PUSHED PROFILES DATAFRAME WITH PROFILES ---- PART 6 \n",
    "    -------------------------------------------------------------------------------------------- '''\n",
    "\n",
    "# Step 4. Join the DataFrames as needed\n",
    "profiles = profiles.join(\n",
    "    profiles_pushed_for_keywords.select(\"id\", \"profile_values_not_sourced\"),\n",
    "    on=\"id\",\n",
    "    how=\"inner\"  # Change join type if needed\n",
    ")\n",
    "\n",
    "display(profiles)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d88fe22-b5be-44ee-946b-480d32b7f3d6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "''' --------------------------------------------------------------------------------------------\n",
    "VALUES FEATURE ON PROFILES DATAFRAME ---- TO DO PART 5 --- VERSION 2 with WORD EMBEDDINGS Extension\n",
    "    -------------------------------------------------------------------------------------------- '''\n",
    "\n",
    "'''import sparknlp\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import ArrayType, StringType, FloatType\n",
    "from sparknlp.base import DocumentAssembler\n",
    "from sparknlp.annotator import Tokenizer, Normalizer, WordEmbeddingsModel\n",
    "from pyspark.ml import Pipeline\n",
    "from scipy.spatial.distance import cosine\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "# Initialize Spark NLP\n",
    "spark = sparknlp.start()\n",
    "\n",
    "# Invert the dictionary: synonym -> set of core values\n",
    "def invert_core_values_dict(core_values_dict):\n",
    "    inverted = defaultdict(set)\n",
    "    for cv, synonyms in core_values_dict.items():\n",
    "        for syn in synonyms:\n",
    "            inverted[syn.lower()].add(cv)  # Ensure lowercase for matching\n",
    "    return dict(inverted)\n",
    "\n",
    "inverted_dict = invert_core_values_dict(core_values_lexical_field)\n",
    "\n",
    "# Prepare Synonym DataFrame\n",
    "synonym_rows = [(synonym, list(core_values)) for synonym, core_values in inverted_dict.items()]\n",
    "synonyms_df = spark.createDataFrame(synonym_rows, [\"text\", \"core_values\"])\n",
    "\n",
    "# Define Spark NLP Pipeline\n",
    "document_assembler = DocumentAssembler() \\\n",
    "    .setInputCol(\"text\") \\\n",
    "    .setOutputCol(\"document\")\n",
    "\n",
    "tokenizer = Tokenizer() \\\n",
    "    .setInputCols([\"document\"]) \\\n",
    "    .setOutputCol(\"token\")\n",
    "\n",
    "normalizer = Normalizer() \\\n",
    "    .setInputCols([\"token\"]) \\\n",
    "    .setOutputCol(\"normalized\") \\\n",
    "    .setLowercase(True)\n",
    "\n",
    "embeddings = WordEmbeddingsModel.pretrained(\"glove_100d\") \\\n",
    "    .setInputCols([\"document\", \"token\"]) \\\n",
    "    .setOutputCol(\"embeddings\")\n",
    "\n",
    "nlp_pipeline = Pipeline(stages=[\n",
    "    document_assembler,\n",
    "    tokenizer,\n",
    "    normalizer,\n",
    "    embeddings\n",
    "])\n",
    "\n",
    "# Fit the pipeline on the synonyms DataFrame\n",
    "nlp_model = nlp_pipeline.fit(synonyms_df)\n",
    "synonyms_with_embeddings = nlp_model.transform(synonyms_df)\n",
    "\n",
    "# Collect Synonym Embeddings into a Python Dictionary\n",
    "synonym_embeddings_map = {}\n",
    "for row in synonyms_with_embeddings.select(\"text\", \"core_values\", \"embeddings\").collect():\n",
    "    if row.embeddings:\n",
    "        embedding_vector = row.embeddings[0].embeddings  # First token embedding\n",
    "        synonym_embeddings_map[row.text] = (embedding_vector, set(row.core_values))\n",
    "\n",
    "# Broadcast the Synonym Embeddings\n",
    "bc_synonym_embeddings_map = spark.sparkContext.broadcast(synonym_embeddings_map)\n",
    "\n",
    "# Define UDF to Find Core Values Using Cosine Similarity\n",
    "def semantic_core_values_finder(sentence_embedding):\n",
    "    \"\"\"\n",
    "    Match sentence embedding to synonym embeddings via cosine similarity.\n",
    "    \"\"\"\n",
    "    if not sentence_embedding:\n",
    "        return []\n",
    "\n",
    "    matched_values = set()\n",
    "    synonym_map = bc_synonym_embeddings_map.value  # Access the broadcasted dictionary\n",
    "\n",
    "    sentence_vector = np.array(sentence_embedding, dtype=\"float32\")\n",
    "\n",
    "    threshold = 0.85 # Threshold for matching HYPER PARAMETER\n",
    "\n",
    "    # Compare with each synonym embedding\n",
    "    for synonym, (syn_vector, core_values) in synonym_map.items():\n",
    "        syn_vector = np.array(syn_vector, dtype=\"float32\")\n",
    "        similarity = 1 - cosine(sentence_vector, syn_vector)\n",
    "\n",
    "        if similarity > threshold:  # Threshold for matching\n",
    "            matched_values.update(core_values)\n",
    "    threshold = 0.85 # Threshold for matching HYPER PARAMETER\n",
    "\n",
    "    # Compare with each synonym embedding\n",
    "    for synonym, (syn_vector, core_values) in synonym_map.items():\n",
    "        syn_vector = np.array(syn_vector, dtype=\"float32\")\n",
    "        similarity = 1 - cosine(sentence_vector, syn_vector)\n",
    "\n",
    "        if similarity > threshold:  # Threshold for matching\n",
    "            matched_values.update(core_values)\n",
    "    return list(matched_values)\n",
    "\n",
    "semantic_core_values_udf = F.udf(semantic_core_values_finder, ArrayType(StringType()))\n",
    "\n",
    "# Join keywords into a single text column for processing\n",
    "profiles = profiles.withColumn(\"text\", F.concat_ws(\" \", \"keywords_agg\"))\n",
    "\n",
    "# Apply the NLP pipeline to the profiles DataFrame\n",
    "profiles_with_embeddings = nlp_model.transform(profiles)\n",
    "\n",
    "# Aggregate Token Embeddings to Sentence-Level Embedding (Mean Embedding)\n",
    "def mean_embeddings(embeddings):\n",
    "    if embeddings:\n",
    "        vectors = np.array([e.embeddings for e in embeddings])\n",
    "        return vectors.mean(axis=0).tolist()\n",
    "    return None\n",
    "\n",
    "mean_embeddings_udf = F.udf(mean_embeddings, ArrayType(FloatType()))\n",
    "profiles_with_embeddings = profiles_with_embeddings.withColumn(\n",
    "    \"sentence_embedding\",\n",
    "    mean_embeddings_udf(F.col(\"embeddings\"))\n",
    ")\n",
    "\n",
    "# Detect Core Values Using the UDF\n",
    "profiles_with_embeddings = profiles_with_embeddings.withColumn(\n",
    "    \"detected_core_values\",\n",
    "    semantic_core_values_udf(F.col(\"sentence_embedding\"))\n",
    ")\n",
    "\n",
    "# Display Results\n",
    "profiles_with_embeddings.select(\"keywords_agg\", \"detected_core_values\").show(truncate=False)'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f07bc962-0012-415b-afd7-e20b3b4c1881",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "''' --------------------------------------------------------------------------------------------\n",
    "                                        ENGINEERED PROFILES\n",
    "    -------------------------------------------------------------------------------------------- '''\n",
    "\n",
    "display(profiles)\n",
    "profiles_engineered = profiles\n",
    "profiles_engineered.write.format(\"parquet\").mode(\"overwrite\").save(\"/mnt/lab94290/results/parquet-data/profiles_engineered_ejt\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d4d41cb9-9f93-4e91-8741-dc2417b8a168",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# COMPANIES PART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c04788e-5483-4ce6-bfee-7f78d4b705b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "'''--------------------------------------------------------------------------------------------\n",
    "                                            COMPANIES\n",
    "------------------------------------------------------------------------------------------------'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "31add825-bd0d-4f0a-8e9f-95a6d4143db6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "'''--------------------------------------------------------------------------------------------\n",
    "                    COMPANIES WITH AT LEAST 20 EMPLOYEES_ID DATAFRAME\n",
    "------------------------------------------------------------------------------------------------'''\n",
    "\n",
    "from pyspark.sql.functions import collect_list, explode, array_distinct, array_union, coalesce, col, lit, count\n",
    "\n",
    "\n",
    "# Assuming 'profiles' is your DataFrame\n",
    "companies_with_employees = (\n",
    "    profiles.groupBy(\"current_company:name\")\n",
    "    .agg(collect_list(\"id\").alias(\"employee_ids\"))\n",
    ")\n",
    "\n",
    "profiles_pushed = profiles.select('id', 'experience')\n",
    "\n",
    "# Step 1: Explode the 'experience' column to get each experience as a separate row\n",
    "exploded_profiles = profiles_pushed.withColumn(\"experience_exploded\", explode(\"experience\"))\n",
    "\n",
    "# Step 2: Extract 'id' and 'subtitle' from the exploded experience column\n",
    "profiles_with_subtitle = exploded_profiles.select(\n",
    "    \"id\", col(\"experience_exploded.subtitle\").alias(\"subtitle\")\n",
    ")\n",
    "\n",
    "# Step 3: Group by 'id' and collect all 'subtitle' values into a list\n",
    "grouped_profiles = profiles_with_subtitle.groupBy(\"id\").agg(\n",
    "    collect_list(\"subtitle\").alias(\"subtitles\")\n",
    ")\n",
    "\n",
    "# Step 1: Explode the 'subtitles' column in 'grouped_profiles'\n",
    "exploded_subtitles = grouped_profiles.withColumn(\"subtitle\", explode(\"subtitles\")).select('id', 'subtitle')\n",
    "\n",
    "# Group by 'subtitle' and collect all 'id' values into a list\n",
    "subtitles_with_ids = exploded_subtitles.groupBy(\"subtitle\").agg(\n",
    "    collect_list(\"id\").alias(\"ids\")\n",
    ")\n",
    "\n",
    "\n",
    "# Perform a full outer join between companies_with_employees and subtitles_with_ids\n",
    "updated_companies_with_employees = companies_with_employees.join(\n",
    "    subtitles_with_ids,\n",
    "    companies_with_employees[\"current_company:name\"] == subtitles_with_ids[\"subtitle\"],\n",
    "    \"full_outer\"\n",
    ")\n",
    "\n",
    "# Merge the 'employee_ids' and 'ids' columns, ensuring unique values\n",
    "companies_employees = updated_companies_with_employees.withColumn(\n",
    "    \"employee_ids\",\n",
    "    array_union(\n",
    "        coalesce(col(\"employee_ids\"), lit([])),  # Replace null with an empty list\n",
    "        coalesce(col(\"ids\"), lit([]))  # Replace null with an empty list\n",
    "    )\n",
    ").select(\n",
    "    coalesce(col(\"current_company:name\"), col(\"subtitle\")).alias(\"company_name\"),\n",
    "    \"employee_ids\"\n",
    ")\n",
    "\n",
    "from pyspark.sql.functions import array_distinct, size, col\n",
    "\n",
    "# Ensure 'employee_ids' contains only distinct values\n",
    "companies_employees = companies_employees.withColumn(\n",
    "    \"employee_ids\", array_distinct(col(\"employee_ids\"))\n",
    ")\n",
    "\n",
    "# Add a column for the size of the 'employee_ids' list\n",
    "companies_employees = companies_employees.withColumn(\n",
    "    \"employee_count\", size(col(\"employee_ids\"))\n",
    ")\n",
    "\n",
    "# Filter companies with at least 20 employees\n",
    "companies_employees = companies_employees.filter(\n",
    "    col(\"employee_count\") >= 20\n",
    ").drop(\"employee_count\")\n",
    "\n",
    "# Drop rows where 'company_name' is null\n",
    "companies_employees = companies_employees.filter(col(\"company_name\").isNotNull())\n",
    "\n",
    "# Only keep companies present in both DataFrames\n",
    "companies_employees = companies_employees.join(\n",
    "    companies,\n",
    "    companies_employees[\"company_name\"] == companies[\"name\"],\n",
    "    \"inner\"  # Inner join to keep only matched companies\n",
    ").select(\"company_name\", \"employee_ids\")\n",
    "\n",
    "# Find duplicated company names by grouping and counting\n",
    "duplicated_companies = companies_employees.groupBy(\"company_name\").agg(\n",
    "    count(\"company_name\").alias(\"count\")\n",
    ").filter(col(\"count\") > 1)\n",
    "\n",
    "# Join with the original DataFrame to display the rows with duplicated company names\n",
    "duplicated_rows = companies_employees.join(\n",
    "    duplicated_companies, on=\"company_name\", how=\"inner\"\n",
    ")\n",
    "\n",
    "# Create a DataFrame with distinct company_name values from duplicated_rows\n",
    "distinct_duplicated_companies = duplicated_rows.select(\"company_name\").distinct()\n",
    "\n",
    "# Perform a left anti-join to drop rows where 'company_name' is in distinct_duplicated_companies\n",
    "companies_employees = companies_employees.join(\n",
    "    distinct_duplicated_companies, on=\"company_name\", how=\"left_anti\"\n",
    ")\n",
    "\n",
    "display(companies_employees)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae85c4cc-7190-407e-995b-143ff5eb898f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "'''--------------------------------------------------------------------------------------------\n",
    "                                        COMPANY PATTERNS\n",
    "------------------------------------------------------------------------------------------------'''\n",
    "\n",
    "from pyspark.sql.functions import explode, col, avg, variance\n",
    "from pyspark.sql.functions import regexp_extract, col, concat_ws\n",
    "from pyspark.sql.functions import (\n",
    "    explode, explode_outer, regexp_extract, when, col, expr, udf, concat_ws, avg, variance, round as spark_round\n",
    ")\n",
    "from pyspark.sql.types import ArrayType, StructType, StructField, StringType, FloatType\n",
    "\n",
    "from pyspark.sql.types import ArrayType, StructType, StructField, StringType, FloatType\n",
    "\n",
    "from pyspark.sql import Window\n",
    "\n",
    "# Explode the 'employee_ids' column in the 'companies_employees' DataFrame\n",
    "exploded_df = companies_employees.withColumn(\"employee_id\", explode(col(\"employee_ids\"))).drop(\"employee_ids\")\n",
    "\n",
    "\n",
    "# Join the exploded DataFrame with the 'profiles' DataFrame\n",
    "patterns_df = exploded_df.join(profiles, exploded_df[\"employee_id\"] == profiles[\"id\"], \"inner\")\n",
    "\n",
    "# Define a window that partitions by 'company_name'\n",
    "window_spec = Window.partitionBy(\"company_name\")\n",
    "\n",
    "\n",
    "# -------- ON VOLUNTEERING --------\n",
    "\n",
    "patterns_df = patterns_df.withColumn(\n",
    "    \"volunteering_percentage\",\n",
    "    avg(col(\"volunteering\")).over(window_spec) * 100\n",
    ")\n",
    "\n",
    "# -------- ON TOP UNIVERSITIES --------\n",
    "\n",
    "patterns_df = patterns_df.withColumn(\n",
    "    \"top_university_percentage\",\n",
    "    avg(col(\"top_university\")).over(window_spec) * 100\n",
    ")\n",
    "\n",
    "# -------- ON AVERAGE POST DURATIONS --------\n",
    "\n",
    "patterns_df = patterns_df.withColumn(\n",
    "    \"mean_post_duration\",\n",
    "    avg(col(\"average_months_of_experience\")).over(window_spec)\n",
    ")\n",
    "\n",
    "patterns_df = patterns_df.withColumn(\n",
    "    \"variance_post_duration\",\n",
    "    variance(col(\"average_months_of_experience\")).over(window_spec)\n",
    ")\n",
    "\n",
    "# -------- ON DEGREES --------\n",
    "\n",
    "# Explode the 'degree' column into individual degrees\n",
    "patterns_df = patterns_df.withColumn(\"degree\", explode_outer(col(\"degrees\")))\n",
    "\n",
    "\n",
    "# Calculate the total number of employees per company\n",
    "patterns_df = patterns_df.withColumn(\n",
    "    \"total_employees\", count(\"degree\").over(window_spec)\n",
    ")\n",
    "\n",
    "# Calculate the count of each degree type per company\n",
    "patterns_df = patterns_df.withColumn(\n",
    "    \"bachelor_count\",\n",
    "    count(when(col(\"degree\") == \"Bachelor\", 1)).over(window_spec),\n",
    ").withColumn(\n",
    "    \"associate_count\",\n",
    "    count(when(col(\"degree\") == \"Associate\", 1)).over(window_spec),\n",
    ").withColumn(\n",
    "    \"master_count\",\n",
    "    count(when(col(\"degree\") == \"Master\", 1)).over(window_spec),\n",
    ").withColumn(\n",
    "    \"no_degree_count\",\n",
    "    count(when(col(\"degree\").isNull() | (col(\"degree\") == \"\"), 1)).over(window_spec),\n",
    ")\n",
    "\n",
    "# Calculate the degree percentages and add it as a new column\n",
    "patterns_df = patterns_df.withColumn(\n",
    "    \"degree_percentage\",\n",
    "    concat_ws(\n",
    "        \", \",\n",
    "        concat_ws(\"\", lit(\"Bachelor: \"), spark_round((col(\"bachelor_count\") * 100.0 / col(\"total_employees\")), 2), lit(\" %\")),\n",
    "        concat_ws(\"\", lit(\"Associate: \"), spark_round((col(\"associate_count\") * 100.0 / col(\"total_employees\")), 2), lit(\" %\")),\n",
    "        concat_ws(\"\", lit(\"Master: \"), spark_round((col(\"master_count\") * 100.0 / col(\"total_employees\")), 2), lit(\" %\")),\n",
    "        concat_ws(\"\", lit(\"No Degree: \"), spark_round((col(\"no_degree_count\") * 100.0 / col(\"total_employees\")), 2), lit(\" %\"))\n",
    "    )\n",
    ")\n",
    "\n",
    "# Drop unnecessary intermediate columns (optional)\n",
    "patterns_df = patterns_df.drop(\"bachelor_count\", \"associate_count\", \"master_count\", \"no_degree_count\", \"total_employees\")\n",
    "patterns_df = patterns_df.dropDuplicates([\"id\"])\n",
    "\n",
    "\n",
    "'''\n",
    "# -------- ON PROFILE PICTURE EMOTIONS --------\n",
    "\n",
    "# Expressions régulières pour extraire les scores\n",
    "happy_regex = r\"score=([\\d\\.]+), label=happy1\"\n",
    "neutral_regex = r\"score=([\\d\\.]+), label=neutral1\"\n",
    "\n",
    "# Ajout des colonnes pour les scores \"happy\" et \"neutral\"\n",
    "patterns_df = patterns_df.withColumn(\"happy_score\", regexp_extract(col(\"avatar_emotions_str\"), happy_regex, 1).cast(\"float\")) \\\n",
    "                 .withColumn(\"neutral_score\", regexp_extract(col(\"avatar_emotions_str\"), neutral_regex, 1).cast(\"float\"))\n",
    "\n",
    "# Calcul des moyennes\n",
    "patterns_df = patterns_df.withColumn(\"avg_happy_avatar\", expr(\"avg(happy_score)\").over(window_spec)) \\\n",
    "                   .withColumn(\"avg_neutral_avatar\", expr(\"avg(neutral_score)\").over(window_spec))\n",
    "# Fusionner l'array en une seule chaîne\n",
    "patterns_df = patterns_df.withColumn(\"avatar_emotions_str\", concat_ws(\", \", col(\"avatar_emotions\")))\n",
    "\n",
    "# Add 'emotions' column based on max of 'happy_score' and 'neutral_score', handling null values\n",
    "patterns_df = patterns_df.withColumn(\n",
    "        \"emotions\",\n",
    "        when((col(\"happy_score\").isNotNull()) & (col(\"neutral_score\").isNotNull()),\n",
    "             when(col(\"happy_score\") > col(\"neutral_score\"), \"happy\").otherwise(\"neutral\"))\n",
    "        .otherwise(None)\n",
    "    )\n",
    "    \n",
    "# Add 'company_emotion' based on max of 'avg_happy_avatar' and 'avg_neutral_avatar', handling null values\n",
    "patterns_df = patterns_df.withColumn(\n",
    "        \"company_emotion\",\n",
    "        when((col(\"avg_happy_avatar\").isNotNull()) & (col(\"avg_neutral_avatar\").isNotNull()),\n",
    "             when(col(\"avg_happy_avatar\") > col(\"avg_neutral_avatar\"), \"happy\").otherwise(\"neutral\"))\n",
    "        .otherwise(None)\n",
    "    )\n",
    "'''\n",
    "\n",
    "display(patterns_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6fcae8ac-fd01-4b9c-a6cf-b7bee82a9e95",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define the target path\n",
    "output_path = \"dbfs:/FileStore/patterns_df_ejt\"\n",
    "\n",
    "# Write the DataFrame in Parquet format (you can change to CSV, JSON, etc.)\n",
    "patterns_df.write.mode(\"overwrite\").parquet(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99bbd3be-148f-4fea-9608-dc642b518792",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# The path\n",
    "input_path = \"dbfs:/FileStore/patterns_df_ejt\"\n",
    "patterns_df = spark.read.parquet(input_path)\n",
    "\n",
    "# Show the first few rows\n",
    "display(patterns_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6bb87242-2bf8-461c-bdb2-66994db2f650",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "'''--------------------------------------------------------------------------------------------\n",
    "                    COMPARABLY SCRAPED FROM SCRAPPED COMPANY WEBSITE \n",
    "------------------------------------------------------------------------------------------------'''\n",
    "# File path\n",
    "file_path = \"/FileStore/tables/companies_scraped-1.csv\"\n",
    "\n",
    "# ----------- POSTER EXAMPLE -----------\n",
    "\n",
    "# Read CSV into a PySpark DataFrame\n",
    "companies_scraped = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .load(file_path)\n",
    "\n",
    "display(companies_scraped)\n",
    "\n",
    "# Remove duplicate rows in the DataFrame\n",
    "companies_scraped = companies_scraped.distinct()\n",
    "\n",
    "# Perform an inner join to keep only matching rows and add the 'about' column\n",
    "companies_text_for_values = companies_scraped.join(\n",
    "    companies, \n",
    "    companies_scraped[\"company_name\"] == companies[\"name\"], \n",
    "    \"inner\"\n",
    ").select(\n",
    "    companies_scraped[\"*\"],  # Keep all columns from companies_scraped\n",
    "    companies[\"about\"],\n",
    "    companies[\"slogan\"]\n",
    ")\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "display(companies_text_for_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b21dde56-1b31-4e04-aff0-a073aaad4927",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "'''--------------------------------------------------------------------------------------------\n",
    "                    EXTRACT KEYWORDS FROM COMPANIES DATAFRAME\n",
    "------------------------------------------------------------------------------------------------'''\n",
    "\n",
    "# ----------------- FROM 'ABOUT' SECTION -----------------\n",
    "\n",
    "# Extract KEYWORDS\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "import nltk\n",
    "\n",
    "# Step 1: Define UDF with NLTK Resource Download and Filters\n",
    "def extract_keywords_nltk(text):\n",
    "    if text is None:\n",
    "        return []\n",
    "\n",
    "    # Ensure NLTK resources are available on executors\n",
    "    nltk.download('punkt', quiet=True)\n",
    "    nltk.download('averaged_perceptron_tagger', quiet=True)\n",
    "    nltk.download('stopwords', quiet=True)\n",
    "\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    from nltk.corpus import stopwords\n",
    "    from nltk import pos_tag\n",
    "\n",
    "    # Tokenize and tag POS\n",
    "    tokens = word_tokenize(text)\n",
    "    pos_tags = pos_tag(tokens)\n",
    "\n",
    "    # Extract nouns and adjectives, lowercase, and filter for letters and hyphens\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    keywords = [\n",
    "        word.lower() for word, pos in pos_tags\n",
    "        if (pos.startswith('NN') or pos.startswith('JJ'))  # Only nouns and adjectives\n",
    "        and word.lower() not in stop_words  # Remove stopwords\n",
    "        and all(c.isalpha() or c == '-' for c in word)  # Allow only letters and hyphens\n",
    "    ]\n",
    "\n",
    "    # Return distinct keywords\n",
    "    return list(set(keywords))\n",
    "\n",
    "# Step 2: Register UDF\n",
    "nltk_udf = udf(extract_keywords_nltk, ArrayType(StringType()))\n",
    "\n",
    "\n",
    "# FROM ABOUT & SLOGAN & ABOUT US SECTIONS\n",
    "companies_text_for_values = companies_text_for_values.withColumn(\"about_keywords\", nltk_udf(companies_text_for_values[\"about\"]))\n",
    "companies_text_for_values = companies_text_for_values.withColumn(\"mvv_keywords\", nltk_udf(companies_text_for_values[\"scraped_data\"]))\n",
    "\n",
    "companies_text_for_values = companies_text_for_values.withColumn(\"slogan_keywords\", nltk_udf(companies_text_for_values[\"slogan\"]))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f53cf88f-8d75-4eab-b1f1-837e8b20bba7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "'''--------------------------------------------------------------------------------------------\n",
    "                    EXTRACT VALUES FROM COMPANIES DATAFRAME - MODEL BASED\n",
    "------------------------------------------------------------------------------------------------'''\n",
    "\n",
    "'''# ------------------------------- MODEL BASED - WITH CONTEXT -------------------------------\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "import torch\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "\n",
    "# Define the label columns and threshold\n",
    "THRESHOLD = 0.3\n",
    "LABEL_COLUMNS = ['Self-direction: thought', 'Self-direction: action', 'Stimulation', 'Hedonism', 'Achievement',\n",
    "                 'Power: dominance', 'Power: resources', 'Face', 'Security: personal', 'Security: societal',\n",
    "                 'Tradition', 'Conformity: rules', 'Conformity: interpersonal', 'Humility', 'Benevolence: caring',\n",
    "                 'Benevolence: dependability', 'Universalism: concern', 'Universalism: nature', 'Universalism: tolerance',\n",
    "                 'Universalism: objectivity']\n",
    "\n",
    "# Define a function to apply the model to a single string\n",
    "def apply_model_to_text(text):\n",
    "    if not text:\n",
    "        return None\n",
    "\n",
    "    # Load the model and tokenizer inside the function\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"tum-nlp/Deberta_Human_Value_Detector\")\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\"tum-nlp/Deberta_Human_Value_Detector\", trust_remote_code=True)\n",
    "\n",
    "    encoding = tokenizer.encode_plus(\n",
    "        text,\n",
    "        add_special_tokens=True,\n",
    "        max_length=512,\n",
    "        return_token_type_ids=False,\n",
    "        padding=\"max_length\",\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt',\n",
    "    )\n",
    "\n",
    "    with torch.no_grad():\n",
    "        prediction = model(encoding[\"input_ids\"], encoding[\"attention_mask\"])\n",
    "        prediction = prediction[\"output\"].flatten().numpy()\n",
    "\n",
    "    # Filter labels based on threshold\n",
    "    results = []\n",
    "    for label, score in zip(LABEL_COLUMNS, prediction):\n",
    "        if score >= THRESHOLD:\n",
    "            results.append(f\"{label}: {score:.4f}\")\n",
    "    return results\n",
    "\n",
    "\n",
    "# Define a UDF for the function\n",
    "@udf(ArrayType(StringType()))\n",
    "def apply_model_udf(text):\n",
    "    result = apply_model_to_text(text)\n",
    "    # Extract the first word before the first colon\n",
    "    extracted_words = [item.split(':')[0] for item in result]\n",
    "    return extracted_words\n",
    "\n",
    "\n",
    "# FROM ABOUT & SLOGAN & ABOUT US SECTIONS\n",
    "companies = companies.withColumn(\"about_context\", apply_model_udf(companies[\"about\"]))\n",
    "companies = companies.withColumn(\"slogan_context\", apply_model_udf(companies[\"slogan\"]))\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7fcc609d-48b3-4b0d-bea4-fb71758936ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "'''--------------------------------------------------------------------------------------------\n",
    "                            GATHER VALUES IN COMPANIES DATAFRAME \n",
    "------------------------------------------------------------------------------------------------'''\n",
    "\n",
    "from pyspark.sql.functions import col, concat, array_distinct, coalesce, lit\n",
    "\n",
    "# Concatenate all six lists into 'keywords_agg' and remove duplicates\n",
    "companies_text_for_values = companies_text_for_values.withColumn(\n",
    "    \"keywords_agg\",\n",
    "        concat(\n",
    "            coalesce(col(\"mvv_keywords\"), lit([])),\n",
    "            coalesce(col(\"about_keywords\"), lit([])),\n",
    "        )\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1637e137-8a6d-4d0b-a880-827f3d253bcc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(companies_text_for_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bbd92298-199f-4074-90b1-e6ae4f4d4d62",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "'''--------------------------------------------------------------------------------------------\n",
    "                            INFER VALUES IN COMPANIES DATAFRAME\n",
    "------------------------------------------------------------------------------------------------'''\n",
    "\n",
    "# ------------------------------- FINAL STEP Infer Companies Core Values -------------------------------\n",
    "\n",
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"omw-1.4\")\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_word(word):\n",
    "    if word:\n",
    "        return word\n",
    "    return lemmatizer.lemmatize(word.lower(), pos='n')\n",
    "\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "def filter_and_map_keywords(keywords, core_values):\n",
    "    matched_values = []\n",
    "    for kw in keywords:\n",
    "        kw_lemma = lemmatize_word(kw)\n",
    "        found_core = None\n",
    "        for core_val, synset in core_values_lexical_field_with_stemmed_and_lemmatized.items():\n",
    "            if kw_lemma in synset:\n",
    "                found_core = core_val\n",
    "                break\n",
    "        if found_core:\n",
    "            matched_values.append(found_core)\n",
    "    return matched_values\n",
    "\n",
    "core_values_lexical_field_with_stemmed_and_lemmatized = get_values_dict()\n",
    "# 1) Broadcast the expanded_core_values dictionary to all workers\n",
    "bc_core_values_lexical_field_with_stemmed_and_lemmatized = spark.sparkContext.broadcast(core_values_lexical_field_with_stemmed_and_lemmatized)\n",
    "\n",
    "# 2) Import PySpark functions and types\n",
    "from pyspark.sql.functions import udf, col, array_union, array_distinct\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "\n",
    "@udf(returnType=ArrayType(StringType()))\n",
    "def filter_map_udf(keywords):\n",
    "    \"\"\"\n",
    "    A PySpark UDF that takes a list of keywords (ArrayType(StringType))\n",
    "    and returns a list of matched core values.\n",
    "    \"\"\"\n",
    "    if keywords is None:\n",
    "        return []\n",
    "    \n",
    "    # Get the dict from broadcast variable\n",
    "    local_expanded_core_values = bc_core_values_lexical_field_with_stemmed_and_lemmatized.value\n",
    "    \n",
    "    # Use your existing function\n",
    "    matched = filter_and_map_keywords(keywords, local_expanded_core_values)\n",
    "    return matched\n",
    "\n",
    "from pyspark.sql.functions import col, array_distinct\n",
    "\n",
    "# Apply the UDF to extract matched core values and ensure distinct values in the list\n",
    "companies_text_for_values = (\n",
    "    companies_text_for_values\n",
    "    .withColumn(\"company_values_not_sourced\", array_distinct(filter_map_udf(col(\"keywords_agg\"))))\n",
    ")\n",
    "\n",
    "companies_values = (\n",
    "    companies_text_for_values\n",
    "    .withColumn(\"company_values_sourced\", array_distinct(filter_map_udf(col(\"slogan_keywords\"))))\n",
    ")\n",
    "\n",
    "display(companies_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "57c558ee-3771-4153-9a25-2b4e7df5487b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "'''--------------------------------------------------------------------------------------------\n",
    "                    LOGO THEME FROM COMPANIES DATAFRAME\n",
    "------------------------------------------------------------------------------------------------'''\n",
    "\n",
    "!pip install pillow scikit-learn\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "\n",
    "import requests\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import ArrayType, IntegerType\n",
    "\n",
    "# Define the Python function to get dominant colors\n",
    "def get_dominant_colors_from_url(url, n_colors=2):\n",
    "    try:\n",
    "        # Download the image from the URL\n",
    "        response = requests.get(url, stream=True)\n",
    "        if response.status_code == 200:\n",
    "            image = Image.open(response.raw).convert(\"RGB\")\n",
    "            image = image.resize((100, 100))  # Resize to speed up processing\n",
    "            image_array = np.array(image).reshape((-1, 3))\n",
    "            \n",
    "            # Apply KMeans clustering to find dominant colors\n",
    "            kmeans = KMeans(n_clusters=n_colors, random_state=0)\n",
    "            kmeans.fit(image_array)\n",
    "            dominant_colors = kmeans.cluster_centers_.astype(int).tolist()  # Convert to list for Spark compatibility\n",
    "            return dominant_colors\n",
    "        else:\n",
    "            return None  # Return None if the image cannot be fetched\n",
    "    except Exception as e:\n",
    "        return None  # Return None if any error occurs\n",
    "\n",
    "# Register the function as a UDF\n",
    "dominant_colors_udf = udf(lambda url: get_dominant_colors_from_url(url), ArrayType(ArrayType(IntegerType())))\n",
    "\n",
    "# Add the 'dominant_colors_logo' column\n",
    "companies_with_colors = companies.withColumn(\"dominant_colors_logo\", dominant_colors_udf(companies[\"logo\"]))\n",
    "\n",
    "# Show the result\n",
    "display(companies_with_colors)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8a74e45e-45ed-48b3-b847-997399b7a1da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# MODEL INTERPRETABILITY BASED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "69322836-4e67-4bbf-9c3f-c0e1293c8830",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "'''--------------------------------------------------------------------------------------------\n",
    "                        IDENTIFYING COMPANY PATTERNS - MODEL INTERPRETABILITY BASED\n",
    "------------------------------------------------------------------------------------------------'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "706656ee-16fa-4959-bb0d-d7db500e810f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "29578a16-6340-4783-ac12-e729d317fa8b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "'''--------------------------------------------------------------------------------------------\n",
    "                            VISUALIZE COMPANIES FEATURES DISTRIBUTIONS\n",
    "------------------------------------------------------------------------------------------------'''\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.sql.functions import col\n",
    "import pandas as pd\n",
    "\n",
    "# Filter out null values in the 'degree' column\n",
    "filtered_df = patterns_df.filter(col(\"degree\").isNotNull())\n",
    "\n",
    "# Group by the 'degree' column and count occurrences\n",
    "degree_counts = filtered_df.groupBy(\"degree\").count().toPandas()\n",
    "\n",
    "# Define the desired order of degrees\n",
    "desired_order = [\"Associate\", \"Bachelor\", \"Master\", \"Doctorate\"]\n",
    "\n",
    "# Reorder the DataFrame based on the desired order\n",
    "degree_counts[\"degree\"] = pd.Categorical(degree_counts[\"degree\"], categories=desired_order, ordered=True)\n",
    "degree_counts = degree_counts.sort_values(\"degree\")\n",
    "\n",
    "# Create the bar chart\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(degree_counts[\"degree\"], degree_counts[\"count\"], color=\"skyblue\", edgecolor=\"black\")\n",
    "\n",
    "# Add labels and title\n",
    "plt.title(\"Distribution of Degree Types\", fontsize=16)\n",
    "plt.xlabel(\"Degree Type\", fontsize=14)\n",
    "plt.ylabel(\"Count\", fontsize=14)\n",
    "\n",
    "# Show the counts on top of each bar\n",
    "for i, count in enumerate(degree_counts[\"count\"]):\n",
    "    plt.text(i, count + 5, str(count), ha=\"center\", fontsize=12)\n",
    "\n",
    "# Display the plot\n",
    "plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0791f2b-b246-45da-bbda-6209c5294c81",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Convert the column 'average_months_of_experience' to a Pandas DataFrame\n",
    "average_months_df = patterns_df.select(\"average_months_of_experience\").toPandas()\n",
    "\n",
    "# Drop null values to avoid errors\n",
    "average_months_df = average_months_df.dropna()\n",
    "\n",
    "# Extract the column as a Pandas Series\n",
    "average_months_series = average_months_df[\"average_months_of_experience\"]\n",
    "\n",
    "# Create the histogram\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(average_months_series, bins=200, edgecolor='black', alpha=0.7)\n",
    "\n",
    "# Add titles and labels\n",
    "plt.title(\"Distribution of Average Months of Professional Experience\", fontsize=16)\n",
    "plt.xlabel(\"Average Months of Experience\", fontsize=14)\n",
    "plt.ylabel(\"Number of Employees\", fontsize=14)\n",
    "\n",
    "# Focus on the range 0 to 600\n",
    "plt.xlim(0, 300)\n",
    "\n",
    "# Add grid lines for better readability\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4587377a-4a6c-489e-a2b9-14b2ce4af74b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Étape 1 : Compter les occurrences des valeurs dans la colonne 'volunteering'\n",
    "counts_df = patterns_df.groupBy(\"top_university\").count()\n",
    "\n",
    "# Étape 2 : Convertir en Pandas DataFrame pour visualisation\n",
    "counts_pandas = counts_df.toPandas()\n",
    "\n",
    "# Étape 3 : Tracer le pie chart\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.pie(\n",
    "    counts_pandas['count'], \n",
    "    labels=counts_pandas['top_university'], \n",
    "    autopct='%1.1f%%', \n",
    "    startangle=90, \n",
    "    colors=['lightblue', 'lightgreen']\n",
    ")\n",
    "plt.title(\"Distribution of the 'top_university' variable\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d1bd7844-a788-491d-90af-758cfa720e67",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Étape 1 : Compter les occurrences des valeurs dans la colonne 'volunteering'\n",
    "counts_df = patterns_df.groupBy(\"volunteering\").count()\n",
    "\n",
    "# Étape 2 : Convertir en Pandas DataFrame pour visualisation\n",
    "counts_pandas = counts_df.toPandas()\n",
    "\n",
    "# Étape 3 : Tracer le pie chart\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.pie(\n",
    "    counts_pandas['count'], \n",
    "    labels=counts_pandas['volunteering'], \n",
    "    autopct='%1.1f%%', \n",
    "    startangle=90, \n",
    "    colors=['lightblue', 'lightgreen']\n",
    ")\n",
    "plt.title(\"Distribution of the 'volunteering' variable\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e5bb1737-bd84-4934-b626-a18630a97754",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "'''VISUALIZATION DISTRIBUTION ALL AVERAGE MONTHS OF EXPERIENCE VS ONE '''\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "def plot_avg_experience_density(company_name):\n",
    "    \"\"\"\n",
    "    Plot the density distribution of average months of experience for a specific company\n",
    "    and for all employees in the dataset.\n",
    "\n",
    "    Args:\n",
    "        company_name (str): The name of the company to analyze.\n",
    "        patterns_df (DataFrame): Spark DataFrame containing 'company_name' and 'average_months_of_experience'.\n",
    "\n",
    "    Returns:\n",
    "        None: Displays the plot.\n",
    "    \"\"\"\n",
    "    # Filter data for the specific company\n",
    "    company_df = patterns_df.filter(col('company_name') == company_name).select(\"average_months_of_experience\")\n",
    "    company_experience = company_df.toPandas()[\"average_months_of_experience\"].dropna()\n",
    "\n",
    "    # Get the overall data\n",
    "    overall_df = patterns_df.select(\"average_months_of_experience\")\n",
    "    overall_experience = overall_df.toPandas()[\"average_months_of_experience\"].dropna()\n",
    "\n",
    "    # Ensure there is sufficient data\n",
    "    if company_experience.empty or overall_experience.empty:\n",
    "        print(f\"Insufficient data for plotting. Check the data for company: {company_name}.\")\n",
    "        return\n",
    "\n",
    "    # Plot the density distributions\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Plot density for the company\n",
    "    sns.kdeplot(company_experience, label=f\"{company_name} Employees\", fill=True, alpha=0.5, color=\"blue\")\n",
    "    \n",
    "    # Plot density for the overall data\n",
    "    sns.kdeplot(overall_experience, label=\"All Employees\", fill=True, alpha=0.5, color=\"green\")\n",
    "\n",
    "    # Add titles and labels\n",
    "    plt.title(\"Density Distribution of Average Months of Experience\", fontsize=16)\n",
    "    plt.xlabel(\"Average Months of Experience\", fontsize=14)\n",
    "    plt.ylabel(\"Density\", fontsize=14)\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "    plt.xlim(0, 300)\n",
    "    # Display the plot\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bae53cb2-a925-496e-849f-95ea0958804b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# STATISTIC TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "84df65aa-ee68-4611-a4ed-4ed0e3f9b1f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "'''--------------------------------------------------------------------------------------------\n",
    "                                        STATISTIC TESTS\n",
    "------------------------------------------------------------------------------------------------'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27df2426-e4bc-413b-8d0b-b92173401594",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "'''TEST STAT FOR VOLUNTEERING'''\n",
    "\n",
    "\n",
    "from statsmodels.stats.proportion import proportions_ztest\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "def test_volunteering_significance(company_name):\n",
    "    \"\"\"\n",
    "    Test if the percentage of volunteering in a specific company is significantly different\n",
    "    from the overall percentage of volunteering across all employees.\n",
    "\n",
    "    Args:\n",
    "        company_name (str): The name of the company to test.\n",
    "        patterns_df (DataFrame): Spark DataFrame containing 'company_name', 'employee_id', and 'volunteering'.\n",
    "\n",
    "    Returns:\n",
    "        dict: Results of the Z-test, including Z-statistic and p-value.\n",
    "    \"\"\"\n",
    "    # Filter data for the specific company\n",
    "    company_df = patterns_df.filter(col('company_name') == company_name)\n",
    "    company_volunteers = company_df.filter(col('volunteering') == 1).count()\n",
    "    company_total = company_df.count()\n",
    "\n",
    "    # Aggregate overall data\n",
    "    total_volunteers = patterns_df.filter(col('volunteering') == 1).count()\n",
    "    total_employees = patterns_df.count()\n",
    "\n",
    "    # Ensure valid data for the test\n",
    "    if company_total == 0 or total_employees == 0:\n",
    "        return {\"error\": \"No data available for the specified company or overall dataset.\"}\n",
    "\n",
    "    # Perform the two-proportion Z-test\n",
    "    count = [company_volunteers, total_volunteers]\n",
    "    nobs = [company_total, total_employees]\n",
    "    stat, p_value = proportions_ztest(count, nobs)\n",
    "\n",
    "    # Interpret the results\n",
    "    alpha = 0.05\n",
    "    significance = \"significant\" if p_value < alpha else \"not significant\"\n",
    "\n",
    "    return {\n",
    "        \"company_name\": company_name,\n",
    "        \"company_volunteers\": company_volunteers,\n",
    "        \"company_total\": company_total,\n",
    "        \"overall_volunteers\": total_volunteers,\n",
    "        \"overall_total\": total_employees,\n",
    "        \"z_statistic\": stat,\n",
    "        \"p_value\": p_value,\n",
    "        \"significance\": significance,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de11edda-cd1e-4e30-b5db-e7e0bac0fd3e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "'''TEST STAT FOR TOP UNIVERSITY'''\n",
    "\n",
    "from statsmodels.stats.proportion import proportions_ztest\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "def test_top_university_significance(company_name):\n",
    "    \"\"\"\n",
    "    Test if the percentage of employees from a 'top university' in a specific company\n",
    "    is significantly different from the overall percentage across all employees.\n",
    "\n",
    "    Args:\n",
    "        company_name (str): The name of the company to test.\n",
    "        patterns_df (DataFrame): Spark DataFrame containing 'company_name', 'employee_id', and 'top_university'.\n",
    "\n",
    "    Returns:\n",
    "        dict: Results of the Z-test, including Z-statistic and p-value.\n",
    "    \"\"\"\n",
    "    # Filter data for the specific company\n",
    "    company_df = patterns_df.filter(col('company_name') == company_name)\n",
    "    company_top_university = company_df.filter(col('top_university') == 1).count()\n",
    "    company_total = company_df.count()\n",
    "\n",
    "    # Aggregate overall data\n",
    "    total_top_university = patterns_df.filter(col('top_university') == 1).count()\n",
    "    total_employees = patterns_df.count()\n",
    "\n",
    "    # Ensure valid data for the test\n",
    "    if company_total == 0 or total_employees == 0:\n",
    "        return {\"error\": \"No data available for the specified company or overall dataset.\"}\n",
    "\n",
    "    # Perform the two-proportion Z-test\n",
    "    count = [company_top_university, total_top_university]\n",
    "    nobs = [company_total, total_employees]\n",
    "    stat, p_value = proportions_ztest(count, nobs)\n",
    "\n",
    "    # Interpret the results\n",
    "    alpha = 0.05\n",
    "    significance = \"significant\" if p_value < alpha else \"not significant\"\n",
    "\n",
    "    return {\n",
    "        \"company_name\": company_name,\n",
    "        \"company_top_university\": company_top_university,\n",
    "        \"company_total\": company_total,\n",
    "        \"overall_top_university\": total_top_university,\n",
    "        \"overall_total\": total_employees,\n",
    "        \"z_statistic\": stat,\n",
    "        \"p_value\": p_value,\n",
    "        \"significance\": significance,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "17ed85d9-322c-4434-b403-7ab029b6a56d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "'''TEST STAT FOR AVERAGE MONTHS OF EXPERIENCE'''\n",
    "\n",
    "\n",
    "from scipy.stats import ttest_ind\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "def test_avg_months_experience_significance(company_name):\n",
    "    \"\"\"\n",
    "    Test if the average months of experience in a specific company is significantly different\n",
    "    from the overall average months of experience across all employees.\n",
    "\n",
    "    Args:\n",
    "        company_name (str): The name of the company to test.\n",
    "        patterns_df (DataFrame): Spark DataFrame containing 'company_name' and 'average_months_of_experience'.\n",
    "\n",
    "    Returns:\n",
    "        dict: Results of the t-test, including t-statistic and p-value.\n",
    "    \"\"\"\n",
    "    # Filter data for the specific company\n",
    "    company_df = patterns_df.filter(col('company_name') == company_name).select(\"average_months_of_experience\")\n",
    "    company_experience = company_df.toPandas()[\"average_months_of_experience\"].dropna()  # Convert to Pandas Series\n",
    "\n",
    "    # Aggregate overall data\n",
    "    overall_df = patterns_df.select(\"average_months_of_experience\")\n",
    "    overall_experience = overall_df.toPandas()[\"average_months_of_experience\"].dropna()  # Convert to Pandas Series\n",
    "\n",
    "    # Ensure there is sufficient data\n",
    "    if len(company_experience) == 0 or len(overall_experience) == 0:\n",
    "        return {\"error\": \"No data available for the specified company or overall dataset.\"}\n",
    "\n",
    "    # Perform the two-sample t-test\n",
    "    stat, p_value = ttest_ind(company_experience, overall_experience, equal_var=False)  # Welch's t-test\n",
    "\n",
    "    # Interpret the results\n",
    "    alpha = 0.05\n",
    "    significance = \"significant\" if p_value < alpha else \"not significant\"\n",
    "\n",
    "    return {\n",
    "        \"company_name\": company_name,\n",
    "        \"company_avg_experience\": company_experience.mean(),\n",
    "        \"company_count\": len(company_experience),\n",
    "        \"overall_avg_experience\": overall_experience.mean(),\n",
    "        \"overall_count\": len(overall_experience),\n",
    "        \"t_statistic\": stat,\n",
    "        \"p_value\": p_value,\n",
    "        \"significance\": significance,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea53fff5-1372-43c9-ba68-776f9920e463",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "'''TEST STAT FOR Doctorate'''\n",
    "\n",
    "\n",
    "from statsmodels.stats.proportion import proportions_ztest\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "def test_doctorate_significance(company_name):\n",
    "    \"\"\"\n",
    "    Test if the proportion of employees with a Doctorate degree in a specific company\n",
    "    is significantly different from the overall proportion across all employees.\n",
    "\n",
    "    Args:\n",
    "        company_name (str): The name of the company to test.\n",
    "        patterns_df (DataFrame): Spark DataFrame containing 'company_name' and 'degree'.\n",
    "\n",
    "    Returns:\n",
    "        dict: Results of the Z-test, including Z-statistic and p-value.\n",
    "    \"\"\"\n",
    "    # Filter data for the specific company\n",
    "    company_df = patterns_df.filter(col('company_name') == company_name)\n",
    "    company_doctorate = company_df.filter(col('degree') == 'Doctorate').count()\n",
    "    company_total = company_df.count()\n",
    "\n",
    "    # Aggregate overall data\n",
    "    total_doctorate = patterns_df.filter(col('degree') == 'Doctorate').count()\n",
    "    total_employees = patterns_df.count()\n",
    "\n",
    "    # Ensure valid data for the test\n",
    "    if company_total == 0 or total_employees == 0:\n",
    "        return {\"error\": \"No data available for the specified company or overall dataset.\"}\n",
    "\n",
    "    # Perform the two-proportion Z-test\n",
    "    count = [company_doctorate, total_doctorate]\n",
    "    nobs = [company_total, total_employees]\n",
    "    stat, p_value = proportions_ztest(count, nobs)\n",
    "\n",
    "    # Interpret the results\n",
    "    alpha = 0.05\n",
    "    significance = \"significant\" if p_value < alpha else \"not significant\"\n",
    "\n",
    "    return {\n",
    "        \"company_name\": company_name,\n",
    "        \"company_doctorate\": company_doctorate,\n",
    "        \"company_total\": company_total,\n",
    "        \"overall_doctorate\": total_doctorate,\n",
    "        \"overall_total\": total_employees,\n",
    "        \"z_statistic\": stat,\n",
    "        \"p_value\": p_value,\n",
    "        \"significance\": significance,\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b6289cdd-9f32-4c51-9d07-b3faea125c07",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "'''TEST STAT FOR MASTER'''\n",
    "\n",
    "\n",
    "from statsmodels.stats.proportion import proportions_ztest\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "def test_master_significance(company_name):\n",
    "    \"\"\"\n",
    "    Test if the proportion of employees with a Master degree in a specific company\n",
    "    is significantly different from the overall proportion across all employees.\n",
    "\n",
    "    Args:\n",
    "        company_name (str): The name of the company to test.\n",
    "        patterns_df (DataFrame): Spark DataFrame containing 'company_name' and 'degree'.\n",
    "\n",
    "    Returns:\n",
    "        dict: Results of the Z-test, including Z-statistic and p-value.\n",
    "    \"\"\"\n",
    "    # Filter data for the specific company\n",
    "    company_df = patterns_df.filter(col('company_name') == company_name)\n",
    "    company_master = company_df.filter(col('degree') == 'Master').count()\n",
    "    company_total = company_df.count()\n",
    "\n",
    "    # Aggregate overall data\n",
    "    total_master = patterns_df.filter(col('degree') == 'Master').count()\n",
    "    total_employees = patterns_df.count()\n",
    "\n",
    "    # Ensure valid data for the test\n",
    "    if company_total == 0 or total_employees == 0:\n",
    "        return {\"error\": \"No data available for the specified company or overall dataset.\"}\n",
    "\n",
    "    # Perform the two-proportion Z-test\n",
    "    count = [company_master, total_master]\n",
    "    nobs = [company_total, total_employees]\n",
    "    stat, p_value = proportions_ztest(count, nobs)\n",
    "\n",
    "    # Interpret the results\n",
    "    alpha = 0.05\n",
    "    significance = \"significant\" if p_value < alpha else \"not significant\"\n",
    "\n",
    "    return {\n",
    "        \"company_name\": company_name,\n",
    "        \"company_master\": company_master,\n",
    "        \"company_total\": company_total,\n",
    "        \"overall_master\": total_master,\n",
    "        \"overall_total\": total_employees,\n",
    "        \"z_statistic\": stat,\n",
    "        \"p_value\": p_value,\n",
    "        \"significance\": significance,\n",
    "    }\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a0010f88-a466-4418-a477-8ae662a55840",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# ALGORITHM INSTRUCTIONS PROMPT - FINAL STEP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd712ae3-17bb-4c97-ae2b-259f5842d437",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "'''--------------------------------------------------------------------------------------------\n",
    "                            ALGORITHM INSTRUCTIONS PROMPT - FINAL STEP\n",
    "------------------------------------------------------------------------------------------------'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a11e4239-503d-4d81-bf62-23bdd717c552",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TEST\n",
    "\n",
    "from pyspark.sql.functions import col, lit, when, expr\n",
    "\n",
    "from pyspark import StorageLevel\n",
    "\n",
    "'''# Persist the DataFrame in memory\n",
    "patterns_df.cache()\n",
    "\n",
    "# Trigger materialization\n",
    "patterns_df.count()'''\n",
    "\n",
    "from pyspark.sql.functions import col, lit, when, expr, explode, collect_set\n",
    "\n",
    "def enhance_profile(profile_id):\n",
    "    \"\"\"\n",
    "    Enhance the profile of an employee by extracting significant features.\n",
    "\n",
    "    Args:\n",
    "        profile_id (str): The ID of the employee profile to analyze.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing only the keys with values.\n",
    "    \"\"\"\n",
    "    # Filtrer les données pour le profil donné\n",
    "    filtered_df = patterns_df.filter(col('employee_id') == profile_id)\n",
    "    \n",
    "    # Initialiser un dictionnaire vide\n",
    "    profile_elements = {\"profile_id\": profile_id}\n",
    "    \n",
    "    if filtered_df.count() > 0:  # Vérifier si le profil existe\n",
    "        row = filtered_df.first()\n",
    "\n",
    "        # Ajouter les éléments standard uniquement s'ils existent\n",
    "        if row[\"degrees\"]:\n",
    "            profile_elements[\"degree\"] = row[\"degrees\"]\n",
    "        \n",
    "        if row[\"average_months_of_experience\"]:\n",
    "            profile_elements[\"average_tenure\"] = row[\"average_months_of_experience\"]\n",
    "        \n",
    "        if row[\"top_university\"] == 1:\n",
    "            profile_elements[\"top_university\"] = row[\"educations_details\"]\n",
    "        \n",
    "        # Ajouter 'picture' si disponible\n",
    "        #emotion_row = filtered_df.select(\"emotions\").first()\n",
    "        #if emotion_row and emotion_row[\"emotions\"]:\n",
    "            #profile_elements[\"picture\"] = emotion_row[\"emotions\"]\n",
    "        \n",
    "        # Ajouter les causes de volontariat si volunteering == 1\n",
    "        if row[\"volunteering\"] == 1:\n",
    "            volunteer_experience_df = filtered_df.withColumn(\n",
    "                \"experience\", explode(col(\"volunteer_experience\"))\n",
    "            ).select(\n",
    "                when(col(\"experience.cause\").isNotNull(), col(\"experience.cause\")).alias(\"cause\")\n",
    "            )\n",
    "            causes = volunteer_experience_df.select(collect_set(\"cause\").alias(\"causes\")).first()[\"causes\"]\n",
    "            if causes:\n",
    "                profile_elements[\"volunteer\"] = causes\n",
    "        \n",
    "        # Ajouter 'values_not_sourced' en fusionnant toutes les sources disponibles\n",
    "        values_from_feature = row[\"values_from_features\"]\n",
    "        if values_from_feature:\n",
    "            values_not_sourced = []\n",
    "            if \"degrees\" in values_from_feature and values_from_feature[\"degrees\"]:\n",
    "                values_not_sourced.extend(values_from_feature[\"degrees\"])\n",
    "            if \"company_and_organization_types\" in values_from_feature and values_from_feature[\"company_and_organization_types\"]:\n",
    "                values_not_sourced.extend(values_from_feature[\"company_and_organization_types\"])\n",
    "            \n",
    "            if values_not_sourced:\n",
    "                profile_elements[\"values_not_sourced\"] = values_not_sourced\n",
    "        \n",
    "        # Ajouter tout dans 'values_sourced' sans hiérarchie\n",
    "        values_sourced = {}\n",
    "        \n",
    "        # Ajouter les valeurs de volontariat\n",
    "        values_from_volunteering = row[\"values_from_volunteering\"]\n",
    "        if values_from_volunteering:\n",
    "            values_sourced.update(values_from_volunteering)\n",
    "        \n",
    "        # Ajouter les valeurs de 'top_university' si existantes\n",
    "        if row[\"top_university\"] == 1:\n",
    "            education_details = row[\"educations_details\"]\n",
    "            if \"top_university\" in values_from_feature and values_from_feature[\"top_university\"]:\n",
    "                values_sourced.update({\n",
    "                    value: (\"top_university\", education_details) for value in values_from_feature[\"top_university\"]\n",
    "                })\n",
    "        \n",
    "        if values_sourced:\n",
    "            profile_elements[\"values_sourced\"] = values_sourced\n",
    "    \n",
    "    return profile_elements\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "986f2778-a4af-4fbd-ac4b-203dd73767fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "from pyspark.sql.functions import col, when\n",
    "\n",
    "def enhance_company(company_name):\n",
    "    \"\"\"\n",
    "    Enhance the profile of a company by testing the significance of various features.\n",
    "\n",
    "    Args:\n",
    "        company_name (str): The name of the company to analyze.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary with significant features and values.\n",
    "    \"\"\"\n",
    "\n",
    "    # Filter the data for the specific company\n",
    "    filtered_df = patterns_df.filter(col('company_name') == company_name)\n",
    "    company_values_df = companies_values.filter(col('company_name') == company_name)\n",
    "    company_colors_df = companies_with_colors.filter(col('company_name') == company_name)\n",
    "\n",
    "    # Initialize a dictionary to store company profile elements\n",
    "    company_elements = {'company_name': company_name}\n",
    "\n",
    "    # Ensure there is data for the given company\n",
    "    if filtered_df.count() > 0:\n",
    "\n",
    "        # Perform all the tests\n",
    "        volunteering_test = test_volunteering_significance(company_name)\n",
    "        top_university_test = test_top_university_significance(company_name)\n",
    "        avg_months_test = test_avg_months_experience_significance(company_name)\n",
    "        doctorate_test = test_doctorate_significance(company_name)\n",
    "        master_test = test_master_significance(company_name)\n",
    "\n",
    "        # List of tests to evaluate\n",
    "        tests = {\n",
    "            \"volunteering\": volunteering_test,\n",
    "            \"top_university\": top_university_test,\n",
    "            \"average_tenure\": avg_months_test,\n",
    "            \"doctorate\": doctorate_test,\n",
    "            \"master\": master_test,\n",
    "        }\n",
    "\n",
    "        # Add significant features and p-values to the dictionary\n",
    "        significant_features = {\n",
    "            feature: result[\"p_value\"]\n",
    "            for feature, result in tests.items()\n",
    "            if \"p_value\" in result and result[\"p_value\"] < 0.05\n",
    "        }\n",
    "\n",
    "        # Sort the significant features by p-value in ascending order (most significant first)\n",
    "        sorted_features = dict(sorted(significant_features.items(), key=lambda x: x[1]))\n",
    "\n",
    "        # Update company_elements in the correct order\n",
    "        company_elements.update(sorted_features)\n",
    "    \n",
    "    # Retrieve values_not_sourced and values_slogan\n",
    "    if company_values_df.count() > 0:\n",
    "        values_slogan_row = company_values_df.select(\"company_values_sourced\").first()\n",
    "        values_not_sourced_row = company_values_df.select(\"company_values_not_sourced\").first()\n",
    "        \n",
    "        company_elements[\"values_not_sourced\"] = values_not_sourced_row[\"company_values_not_sourced\"] if values_not_sourced_row else []\n",
    "        company_elements[\"values_slogan\"] = values_slogan_row[\"company_values_sourced\"] if values_slogan_row else []\n",
    "    else:\n",
    "        company_elements[\"values_not_sourced\"] = []\n",
    "        company_elements[\"values_slogan\"] = []\n",
    "\n",
    "    # Retrieve 'picture' from 'company_emotion'\n",
    "    #company_emotion_row = filtered_df.select(\"company_emotion\").first()\n",
    "    #company_elements[\"picture\"] = company_emotion_row.company_emotion if company_emotion_row else None\n",
    "    \n",
    "    # Retrieve 'logo_theme' from 'dominant_colors_logo' and convert it to a string\n",
    "    logo_theme_row = company_colors_df.select(\"dominant_colors_logo\").first()\n",
    "    company_elements[\"logo_theme\"] = [str(color) for color in logo_theme_row[\"dominant_colors_logo\"]] if logo_theme_row and logo_theme_row[\"dominant_colors_logo\"] else []\n",
    "    \n",
    "    return company_elements\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9596fbad-2a08-4aa0-ade3-0e194c1a4755",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#                                                           THE FINAL FUNCTIONS\n",
    "# ----------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "def get_matching(profile_elements, company_elements, profile_id, company_name):\n",
    "\n",
    "    # ------------------- VALUES -------------------\n",
    "    # Initialize matched_elements with 'values' as an empty list.\n",
    "    matched_elements = {'values': []}\n",
    "    \n",
    "    # Check if the company has a non-empty 'values_slogan' list.\n",
    "    if 'values_slogan' in company_elements and company_elements['values_slogan']:\n",
    "        for slogan in company_elements['values_slogan']:\n",
    "            # If the slogan is in the profile's 'values_not_sourced' list, append a tuple.\n",
    "            if slogan in profile_elements.get('values_not_sourced', []):\n",
    "                matched_elements['values'].append((slogan, None, 'slogan'))\n",
    "            \n",
    "            # If the slogan is a key in profile_elements['values_sourced'],\n",
    "            # retrieve the first element of the first inner list and append a tuple.\n",
    "            if slogan in profile_elements.get('values_sourced', {}):\n",
    "                sourced_value = profile_elements['values_sourced'][slogan]\n",
    "                # Verify that sourced_value is a non-empty list and its first element is also non-empty.\n",
    "                if sourced_value and isinstance(sourced_value, list) and len(sourced_value) > 0 and sourced_value[0]:\n",
    "                    first_inner_list = sourced_value[0]\n",
    "                    if isinstance(first_inner_list, list) and len(first_inner_list) > 0:\n",
    "                        first_element = first_inner_list[0]\n",
    "                        matched_elements['values'].append((slogan, first_element, 'slogan'))\n",
    "      \n",
    "    # Process values_not_sourced from company_elements.\n",
    "    if 'values_not_sourced' in company_elements and company_elements['values_not_sourced']:\n",
    "        for value in company_elements['values_not_sourced']:\n",
    "            # If the value is in profile's values_not_sourced, append (value, None, None).\n",
    "            if value in profile_elements.get('values_not_sourced', []):\n",
    "                matched_elements['values'].append((value, None, None))\n",
    "            \n",
    "            # If the value is a key in profile's values_sourced, append (value, first_element, None).\n",
    "            if value in profile_elements.get('values_sourced', {}):\n",
    "                sourced_value = profile_elements['values_sourced'][value]\n",
    "                if (sourced_value and isinstance(sourced_value, list) and len(sourced_value) > 0 \n",
    "                        and sourced_value[0] and isinstance(sourced_value[0], list) and len(sourced_value[0]) > 0):\n",
    "                    first_element = sourced_value[0][0]\n",
    "                    matched_elements['values'].append((value, first_element, None))\n",
    "\n",
    "                  \n",
    "      # Deduplicate tuples based on the first element.\n",
    "    # For each key, keep the tuple with fewer None values in its second and third positions.\n",
    "    best = {}\n",
    "    for tpl in matched_elements['values']:\n",
    "        key = tpl[0]\n",
    "        # Count the number of None values in positions 1 and 2.\n",
    "        none_count = (1 if tpl[1] is None else 0) + (1 if tpl[2] is None else 0)\n",
    "        if key not in best:\n",
    "            best[key] = tpl\n",
    "        else:\n",
    "            existing = best[key]\n",
    "            existing_none_count = (1 if existing[1] is None else 0) + (1 if existing[2] is None else 0)\n",
    "            # If the current tuple has fewer None values, replace the existing tuple.\n",
    "            if none_count < existing_none_count:\n",
    "                best[key] = tpl\n",
    "    \n",
    "    # Replace the values list with the deduplicated tuples.\n",
    "    matched_elements['values'] = list(best.values())\n",
    "\n",
    "    # ------------------- PATTERNS -------------------\n",
    "    \n",
    "    # Initialize matched_elements with 'patterns' as an empty list.\n",
    "    matched_elements['patterns'] = []\n",
    "\n",
    "    # Rank specific numerical columns in ascending order.\n",
    "    rank_columns = ['master', 'doctorate', 'volunteer', 'average_tenure', 'top_university']\n",
    "    filtered_columns = {col: company_elements[col] for col in rank_columns if col in company_elements and isinstance(company_elements[col], (int, float))}\n",
    "    \n",
    "    # Sort column names by their values in ascending order.\n",
    "    company_patterns = sorted(filtered_columns, key=filtered_columns.get)\n",
    "\n",
    "    # Process each pattern in the sorted list.\n",
    "    for pattern in company_patterns:\n",
    "        # For the volunteering case:\n",
    "        # Note: Company uses the key \"volunteering\" (in rank_columns), but profile_elements holds \"volunteer\".\n",
    "        if pattern == 'volunteer' and 'volunteer' in profile_elements and len(profile_elements['volunteer'])>0 :\n",
    "            matched_elements['patterns'].append('volunteer')\n",
    "        \n",
    "        # For the top_university case:\n",
    "        if pattern == 'top_university' and 'top_university' in profile_elements and profile_elements['top_university']:\n",
    "            # Append a string with the value from profile_elements inserted.\n",
    "            matched_elements['patterns'].append(\n",
    "                \"top_university:{}\".format(profile_elements['top_university'])\n",
    "            )\n",
    "\n",
    "        # For the master case:\n",
    "        # If in profile_elements['degree'] there is 'master', append 'master'\n",
    "        # but only if 'doctorate' is not already in the patterns.\n",
    "        if pattern == 'master':\n",
    "            if 'degree' in profile_elements and 'Master' in profile_elements['degree']:\n",
    "                if 'doctorate' not in matched_elements['patterns']:\n",
    "                    matched_elements['patterns'].append('master')\n",
    "        \n",
    "        # For the doctorate case:\n",
    "        # If in profile_elements['degree'] there is 'doctorate', append 'doctorate'\n",
    "        # and remove 'master' from patterns if it exists.\n",
    "        if pattern == 'doctorate':\n",
    "            if 'degree' in profile_elements and 'Doctorate' in profile_elements['degree']:\n",
    "                matched_elements['patterns'].append('doctorate')\n",
    "                if 'master' in matched_elements['patterns']:\n",
    "                    matched_elements['patterns'].remove('master')\n",
    "        \n",
    "        # ADD AVERAGE TENURE \n",
    "\n",
    "    # ------------------- SYNERGIES -------------------\n",
    "\n",
    "    if 'picture' in company_elements and 'picture' in profile_elements:\n",
    "        if company_elements['picture'] == profile_elements['picture']:\n",
    "            matched_elements['picture'] = [company_elements['picture'], 1]\n",
    "        else:\n",
    "            matched_elements['picture'] = [company_elements['picture'], 0]\n",
    "\n",
    "    matched_elements['theme'] = company_elements['logo_theme']\n",
    "\n",
    "\n",
    "    return matched_elements\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d1d00394-6b55-4183-a832-132990796fb7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ------------------- BOOKS -------------------\n",
    "\n",
    "def get_books():\n",
    "    # CORE VALUES BOOKS\n",
    "    core_values_ps_cs = [\"According to {company source}, {core value} is a key focus at {company name}; referencing {profile source} will help emphasize your {core value}.\",\n",
    "    \"From {company source}, it's clear that {core value} is highly valued at {company name}; incorporate {profile source} to showcase your {core value}.\",\n",
    "    \"As noted in {company source}, {core value} plays an essential role at {company name}; mentioning {profile source} will reinforce your {core value}.\",\n",
    "    \"Insights from {company source} indicate that {core value} is a core principle at {company name}; referring to {profile source} will underline your {core value}.\",\n",
    "    \"Since {company source} highlights {core value} as a priority for {company name}, make sure to include {profile source} to demonstrate your {core value}.\",\n",
    "    \"Given that {company source} emphasizes {core value} at {company name}, mentioning {profile source} will help reflect your {core value}.\",\n",
    "    \"As per {company source}, {core value} is central to {company name}; referencing {profile source} will effectively showcase your {core value}.\",\n",
    "    \"According to {company source}, {core value} is a defining attribute of {company name}; use {profile source} to strengthen your {core value}.\",\n",
    "    \"The data from {company source} shows that {core value} is integral to {company name}; incorporating {profile source} will highlight your {core value}.\",\n",
    "    \"As seen in {company source}, {core value} is highly regarded at {company name}; including {profile source} will help illustrate your {core value}.\"\n",
    "]\n",
    "\n",
    "    core_values_ps_cns = [\"Make sure to mention {profile source} to emphasize your {core value} and align with {company name}'s expected core values.\",\n",
    "    \"Including {profile source} will help highlight your {core value} in line with {company name}'s core values.\",\n",
    "    \"Be sure to reference {profile source} to showcase your {core value} and match {company name}'s expected core values.\",\n",
    "    \"Mention {profile source} to demonstrate your {core value} and ensure alignment with {company name}'s core values.\",\n",
    "    \"To reflect {company name}'s core values, highlight your {core value} by mentioning {profile source}.\",\n",
    "    \"Use {profile source} to illustrate your {core value} and align with the expectations of {company name}.\",\n",
    "    \"Referencing {profile source} will reinforce your {core value} in accordance with {company name}'s expected core values.\",\n",
    "    \"Showcase your {core value} by mentioning {profile source}, ensuring alignment with {company name}'s core values.\",\n",
    "    \"By including {profile source}, you can emphasize your {core value} in a way that aligns with {company name}'s expectations.\",\n",
    "    \"Ensure your {core value} is highlighted by referencing {profile source}, in line with {company name}'s expected core values.\"\n",
    "]\n",
    "\n",
    "    core_values_pns_cs = [\"Based on {company source}, we observed that {core value} is essential for {company name}; emphasize your {core value} to align with {company name}'s expected core values.\",\n",
    "    \"From {company source}, it's clear that {core value} matters to {company name}; make sure to showcase your {core value} accordingly.\",\n",
    "    \"As indicated by {company source}, {core value} plays a key role at {company name}; ensure your {core value} is highlighted to align with their expectations.\",\n",
    "    \"We gathered from {company source} that {core value} holds significance for {company name}; reinforcing your {core value} will help align with their core values.\",\n",
    "    \"According to {company source}, {core value} is highly valued at {company name}; be sure to highlight your {core value} to match their expectations.\",\n",
    "    \"Insights from {company source} suggest that {core value} is a priority for {company name}; emphasize your {core value} for alignment with their values.\",\n",
    "    \"Since {company source} indicates that {core value} is central to {company name}, ensure that your {core value} stands out in alignment with their core values.\",\n",
    "    \"We noticed from {company source} that {core value} is a fundamental aspect at {company name}; make sure to reflect this by highlighting your {core value}.\",\n",
    "    \"Given that {company source} highlights {core value} as a key value at {company name}, reinforcing your {core value} will strengthen alignment.\",\n",
    "    \"Our analysis of {company source} shows that {core value} is integral to {company name}; be sure to showcase your {core value} to align with their values.\"\n",
    "]\n",
    "\n",
    "    core_values_not_sourced_book = [\n",
    "        \"Emphasize your {core value} in a way that resonates with {company name}’s commitment to {core value}.\",\n",
    "        \"Showcase your {core value} as it aligns with {company name}’s strong emphasis on {core value}.\",\n",
    "        \"Demonstrate your {core value} to reflect {company name}’s dedication to {core value}.\",\n",
    "        \"Illustrate how your {core value} complements {company name}’s appreciation for {core value}.\",\n",
    "        \"Underline your {core value} as it mirrors {company name}’s focus on {core value}.\",\n",
    "        \"Bring attention to your {core value} as it matches {company name}’s prioritization of {core value}.\",\n",
    "        \"Make your {core value} stand out by aligning it with {company name}’s perspective on {core value}.\",\n",
    "        \"Express your {core value} in a way that reinforces {company name}’s belief in {core value}.\",\n",
    "        \"Communicate your {core value} in a manner that supports {company name}’s recognition of {core value}.\",\n",
    "        \"Position your {core value} as a key strength that aligns with {company name}’s values around {core value}.\",\n",
    "    ]\n",
    "\n",
    "    # PATTERNS BOOKS\n",
    "    volunteering_book = [\"We observed that {company name}'s employees are significantly involved in volunteering; mention your volunteering experience.\",\n",
    "    \"It is clear that {company name}'s employees participate in volunteering activities significantly; highlight your volunteering experience.\",\n",
    "    \"We noticed that volunteering is prevalent among {company name}'s employees; be sure to mention your volunteering experience.\",\n",
    "    \"Employees at {company name} have a strong presence in volunteering activities; include your volunteering experience.\",\n",
    "    \"Volunteering appears to be common among {company name}'s employees; reference your own volunteering experience.\",\n",
    "    \"Our observations indicate that {company name}'s employees are highly active in volunteering; mention your experience.\",\n",
    "    \"It has been noted that {company name}'s employees engage in volunteering at a significant rate; showcase your own volunteering efforts.\",\n",
    "    \"We noticed that employees at {company name} often participate in volunteering activities; include your volunteering experience.\",\n",
    "    \"Volunteering is significantly present in the activities of {company name}'s employees; mention your own volunteering experience.\",\n",
    "    \"We found that volunteering is a notable activity among employees of {company name}; highlight your own involvement.\"\n",
    "]\n",
    "\n",
    "    top_univ_book = [\"Leverage your {university name} background to build connections with {company name} employees who studied at top universities in a meaningful way.\",\n",
    "        \"Utilize your {university name} education to establish strong networking ties with {company name} employees from top universities.\",\n",
    "        \"Make use of your {university name} background to connect with {company name} employees who have attended top universities in a valuable way.\",\n",
    "        \"Capitalize on your {university name} background to foster relationships with {company name} employees from prestigious institutions.\",\n",
    "        \"Tap into your {university name} background to create meaningful professional connections with {company name} employees from top universities.\",\n",
    "        \"Use your {university name} experience to build a robust network with {company name} employees who have studied at leading institutions.\",\n",
    "        \"Draw on your {university name} background to form impactful relationships with {company name} employees educated at top universities.\",\n",
    "        \"Leverage your {university name} credentials to engage with {company name} employees who have attended elite universities.\",\n",
    "        \"Harness your {university name} background to develop professional relationships with {company name} employees from world-class universities.\",\n",
    "        \"Take advantage of your {university name} background to strengthen your network with {company name} employees from top-tier institutions.\"\n",
    "    ]\n",
    "\n",
    "    avg_tenure_book = []\n",
    "\n",
    "    master_book = [\"We observed that a significant number of {company name}'s employees hold a master’s degree; mention your master’s degree.\",\n",
    "    \"It is evident that many employees at {company name} have a master’s degree; highlight your own master’s degree.\",\n",
    "    \"Our analysis shows that holding a master’s degree is common among {company name}'s employees; be sure to mention yours.\",\n",
    "    \"We found that a large proportion of {company name}'s employees have a master’s degree; include your master’s degree in your profile.\",\n",
    "    \"Employees at {company name} frequently hold master’s degrees; make sure to highlight yours.\",\n",
    "    \"Holding a master’s degree appears to be common among {company name}'s employees; reference your master’s degree.\",\n",
    "    \"We noticed that master’s degrees are significantly present among {company name}'s employees; be sure to mention yours.\",\n",
    "    \"A substantial number of employees at {company name} have a master’s degree; include your master’s degree in your profile.\",\n",
    "    \"Our observations indicate that many {company name} employees hold a master’s degree; make sure to showcase yours.\",\n",
    "    \"It is clear that master’s degrees are prevalent at {company name}; highlight your own master’s degree.\"\n",
    "]\n",
    "\n",
    "    doctorate_book = [\"We observed that a significant number of {company name}'s employees hold a doctorate degree; mention your doctorate degree.\",\n",
    "    \"It is evident that many employees at {company name} have a doctorate degree; highlight your own doctorate degree.\",\n",
    "    \"Our analysis shows that holding a doctorate degree is common among {company name}'s employees; be sure to mention yours.\",\n",
    "    \"We found that a large proportion of {company name}'s employees have a doctorate degree; include your doctorate degree in your profile.\",\n",
    "    \"Employees at {company name} frequently hold doctorate degrees; make sure to highlight yours.\",\n",
    "    \"Holding a doctorate degree appears to be common among {company name}'s employees; reference your doctorate degree.\",\n",
    "    \"We noticed that doctorate degrees are significantly present among {company name}'s employees; be sure to mention yours.\",\n",
    "    \"A substantial number of employees at {company name} have a doctorate degree; include your doctorate degree in your profile.\",\n",
    "    \"Our observations indicate that many {company name} employees hold a doctorate degree; make sure to showcase yours.\",\n",
    "    \"It is clear that doctorate degrees are prevalent at {company name}; highlight your own doctorate degree.\"\n",
    "]\n",
    "\n",
    "\n",
    "    # SYNERGIES BOOKS\n",
    "    logo_scheme_book = [\"The company's color theme is {colors}, so we recommend choosing a matching background for your resume.\",\n",
    "        \"Since the company's color palette is {colors}, we suggest selecting a background that aligns with these colors for your resume.\",\n",
    "        \"To maintain consistency with the company's theme of {colors}, consider using a resume background that complements these colors.\",\n",
    "        \"The company's branding revolves around {colors}, so we advise you to pick a resume background that harmonizes with these tones.\",\n",
    "        \"With {colors} as the company's primary colors, we suggest choosing a resume background that blends well with them.\"\n",
    "    ]\n",
    "\n",
    "    profile_picture_to_keep_book = [\"Your photo expresses {dominant profile emotion} emotions, matching the tone present in many {company name} employees' profiles. We recommend keeping it as is.\",\n",
    "        \"Your photo reflects {dominant profile emotion} emotions, aligning well with the overall tone of {company name} employees' profiles. We suggest leaving it unchanged.\",\n",
    "        \"The {dominant profile emotion} emotions in your photo resonate with the tone commonly seen in {company name} employees' profiles. Keeping it as is is a good choice.\",\n",
    "        \"Your photo captures {dominant profile emotion} emotions, harmonizing with the tone present in numerous {company name} employees' profiles. No changes are needed.\",\n",
    "        \"The {dominant profile emotion} emotions in your photo are consistent with the tone found in many {company name} employees' profiles. We advise keeping it as is.\",\n",
    "        \"Your photo conveys {dominant profile emotion} emotions that align naturally with the tone of {company name} employees' profiles. No modifications are necessary.\",\n",
    "        \"Your photo showcases {dominant profile emotion} emotions, reflecting the same tone seen in many {company name} employees' profiles. It’s best to leave it as is.\",\n",
    "        \"With its {dominant profile emotion} emotions, your photo aligns with the typical tone in {company name} employees' profiles. We recommend not changing it.\",\n",
    "        \"The {dominant profile emotion} emotions in your photo are in sync with the general tone of {company name} employees' profiles. We suggest keeping it unchanged.\",\n",
    "        \"Your photo’s {dominant profile emotion} emotions complement the tone found across many {company name} employees' profiles. Keeping it as is would be ideal.\"\n",
    "    ]\n",
    "\n",
    "    profile_picture_to_change_book = [\"Your photo conveys {dominant profile emotion} emotions, which contrast with the tone found in many {company name} employees' profiles that reflect {dominant company emotion}. We recommend changing it for a more {dominant company emotion} picture.\",\n",
    "        \"The {dominant profile emotion} emotions in your photo stand out against the {dominant company emotion} tone seen in many {company name} employees' profiles. We suggest updating it to a more {dominant company emotion} picture.\",\n",
    "        \"Your photo expresses {dominant profile emotion} emotions, differing from the {dominant company emotion} tone commonly found in {company name} employees' profiles. Consider changing it to better align with a {dominant company emotion} look.\",\n",
    "        \"Since your photo conveys {dominant profile emotion} emotions, it contrasts with the {dominant company emotion} tone found in {company name} employees' profiles. We advise selecting a more {dominant company emotion} picture.\",\n",
    "        \"The emotions in your photo, which reflect {dominant profile emotion}, do not align with the {dominant company emotion} tone in many {company name} employees' profiles. We recommend adjusting it to a more {dominant company emotion} image.\",\n",
    "        \"Your current photo conveys {dominant profile emotion} emotions, whereas most {company name} employees’ profiles exhibit a {dominant company emotion} tone. You may want to switch to a more {dominant company emotion} picture.\",\n",
    "        \"As your photo reflects {dominant profile emotion} emotions, it contrasts with the {dominant company emotion} tone found in {company name} employees' profiles. We suggest updating it for a more {dominant company emotion} appearance.\",\n",
    "        \"Your {dominant profile emotion} expression differs from the {dominant company emotion} tone present in {company name} employees' profiles. We recommend choosing a more {dominant company emotion} image.\",\n",
    "        \"The {dominant profile emotion} emotions in your photo contrast with the {dominant company emotion} tone across many {company name} employees' profiles. A more {dominant company emotion} picture would be a better fit.\",\n",
    "        \"Your photo’s {dominant profile emotion} emotions are not in sync with the {dominant company emotion} tone of {company name} employees' profiles. Consider changing it for a more {dominant company emotion} look.\"\n",
    "    ]\n",
    "\n",
    "    books = {\n",
    "        'core_values_ps_cs': core_values_ps_cs,\n",
    "        'core_values_ps_cns': core_values_ps_cns,\n",
    "        'core_values_pns_cs': core_values_pns_cs,\n",
    "        'core_values_not_sourced_book': core_values_not_sourced_book,\n",
    "\n",
    "        'volunteering_book': volunteering_book,\n",
    "        'top_univ_book': top_univ_book,\n",
    "        'avg_tenure_book': avg_tenure_book,\n",
    "        'master_book': master_book,\n",
    "        'doctorate_book': doctorate_book,\n",
    "        \n",
    "\n",
    "        'logo_scheme_book': logo_scheme_book,\n",
    "        'profile_picture_to_keep_book': profile_picture_to_keep_book,\n",
    "        'profile_picture_to_change_book': profile_picture_to_change_book\n",
    "    }\n",
    "\n",
    "    return books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e76f23f3-94dd-4469-af46-bc583768d617",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------------------------------------------------------------\n",
    "#                                               LLM PARAPHRASER\n",
    "# ---------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# !!! ENTER YOUR HUGGING FACE TOKEN HERE !!!\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "device = \"cuda\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Ateeqq/Text-Rewriter-Paraphraser\", token='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')\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"Ateeqq/Text-Rewriter-Paraphraser\", token='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').to(device)\n",
    "\n",
    "def generate_title(text):\n",
    "    input_ids = tokenizer(f'paraphraser: {text}', return_tensors=\"pt\", padding=\"longest\", truncation=True, max_length=64).input_ids.to(device)\n",
    "    outputs = model.generate(\n",
    "        input_ids,\n",
    "        num_beams=4,\n",
    "        num_beam_groups=4,\n",
    "        num_return_sequences=1,\n",
    "        repetition_penalty=10.0,\n",
    "        diversity_penalty=3.0,\n",
    "        no_repeat_ngram_size=2,\n",
    "        temperature=0.8,\n",
    "        max_length=64\n",
    "    )\n",
    "    return tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "\n",
    "def paraphraseLLM(prompt):\n",
    "  return generate_title(prompt)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "770977b0-86c2-4e23-9218-f45d30bf5903",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------------------------------------------------------------\n",
    "#                                                     THE INSTRUCTION PDF FUNCTIONS\n",
    "# ---------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "import random\n",
    "\n",
    "def generate_instructions(profile_id, company_name, profile_elements, company_elements):\n",
    "    matched_elements = get_matching(profile_elements, company_elements, profile_id, company_name)\n",
    "\n",
    "    books = get_books()\n",
    "\n",
    "    values_prompts = []\n",
    "    patterns_prompts = []\n",
    "    picture_prompts = []\n",
    "    theme_prompts = []\n",
    "\n",
    "    \n",
    "    # VALUES\n",
    "\n",
    "    # Sort the tuples in matched_elements['values'] by the number of non-None entries in positions 1 and 2, in descending order.\n",
    "    sorted_values = sorted(\n",
    "        matched_elements.get('values', []),\n",
    "        key=lambda tpl: sum(1 for i in (tpl[1], tpl[2]) if i is not None),\n",
    "        reverse=True\n",
    "    )\n",
    "    \n",
    "    # Iterate over the sorted values.\n",
    "    for value_tuple in sorted_values:\n",
    "        core_value = value_tuple[0]\n",
    "        second_entry, third_entry = value_tuple[1], value_tuple[2]\n",
    "\n",
    "        if second_entry is not None and third_entry is not None:\n",
    "            sampled_prompt = random.choice(books['core_values_ps_cs'])\n",
    "            # Perform the replacements\n",
    "            sampled_prompt = (sampled_prompt\n",
    "                          .replace(\"{company source}\", third_entry)\n",
    "                          .replace(\"{core value}\", core_value)\n",
    "                          .replace(\"{company name}\", company_name)\n",
    "                          .replace(\"{profile source}\", second_entry))\n",
    "            reformulation = paraphraseLLM(sampled_prompt)\n",
    "            values_prompts.append(reformulation)\n",
    "        \n",
    "        elif second_entry is None and third_entry is not None:\n",
    "            sampled_prompt = random.choice(books['core_values_pns_cs'])\n",
    "            # Perform the replacements\n",
    "            sampled_prompt = (sampled_prompt\n",
    "                          .replace(\"{company source}\", third_entry)\n",
    "                          .replace(\"{core value}\", core_value)\n",
    "                          .replace(\"{company name}\", company_name))\n",
    "            reformulation = paraphraseLLM(sampled_prompt)\n",
    "            values_prompts.append(reformulation)\n",
    "\n",
    "        elif second_entry is not None and third_entry is None:\n",
    "            sampled_prompt = random.choice(books['core_values_ps_cns'])\n",
    "            # Perform the replacements\n",
    "            sampled_prompt = (sampled_prompt\n",
    "                          .replace(\"{core value}\", core_value)\n",
    "                          .replace(\"{company name}\", company_name)\n",
    "                          .replace(\"{profile source}\", second_entry))\n",
    "            reformulation = paraphraseLLM(sampled_prompt)\n",
    "            values_prompts.append(reformulation)\n",
    "\n",
    "        elif second_entry is None and third_entry is None:\n",
    "            sampled_prompt = random.choice(books['core_values_not_sourced_book'])\n",
    "            # Perform the replacements\n",
    "            sampled_prompt = (sampled_prompt\n",
    "                          .replace(\"{core value}\", core_value)\n",
    "                          .replace(\"{company name}\", company_name))\n",
    "            reformulation = paraphraseLLM(sampled_prompt)\n",
    "            values_prompts.append(reformulation)\n",
    "        \n",
    "    # PATTERNS \n",
    "\n",
    "    # Iterate over matched_elements['patterns'] and sample from respective books.\n",
    "    for pattern in matched_elements.get('patterns', []):\n",
    "        if pattern == 'doctorate':\n",
    "          sampled_prompt = random.choice(books['doctorate_book'])\n",
    "          # Perform the replacements\n",
    "          sampled_prompt = (sampled_prompt\n",
    "                          .replace(\"{company name}\", company_name))\n",
    "          reformulation = paraphraseLLM(sampled_prompt)\n",
    "          patterns_prompts.append(reformulation)\n",
    "\n",
    "        elif pattern == 'master':\n",
    "          sampled_prompt = random.choice(books['master_book'])\n",
    "          # Perform the replacements\n",
    "          sampled_prompt = (sampled_prompt\n",
    "                          .replace(\"{company name}\", company_name))\n",
    "          reformulation = paraphraseLLM(sampled_prompt)\n",
    "          patterns_prompts.append(reformulation)\n",
    "\n",
    "        elif pattern == 'volunteer':  # Fixing typo from 'volunteering_book' to 'volunteer'\n",
    "          sampled_prompt = random.choice(books['volunteering_book'])\n",
    "          # Perform the replacements\n",
    "          sampled_prompt = (sampled_prompt\n",
    "                          .replace(\"{company name}\", company_name))\n",
    "          reformulation = paraphraseLLM(sampled_prompt)\n",
    "          patterns_prompts.append(reformulation)\n",
    "\n",
    "        elif pattern.startswith('top_university'):\n",
    "          _, univ_detail = pattern.split(\":\", 1)  # Splitting at the first colon to get univ_detail\n",
    "          sampled_prompt = random.choice(books['top_univ_book'])\n",
    "          sampled_prompt = sampled_prompt.replace(\"{university name}\", univ_detail)\n",
    "          sampled_prompt = sampled_prompt.replace(\"{company name}\", company_name)\n",
    "          reformulation = paraphraseLLM(sampled_prompt)\n",
    "          patterns_prompts.append(reformulation)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # PICTURE AND THEME \n",
    "\n",
    "    if matched_elements.get('picture', [])[1] == 1:\n",
    "        profile_emotion = matched_elements.get('picture', [])[0]\n",
    "        sampled_prompt = random.choice(books['profile_picture_to_keep_book'])\n",
    "        # Perform the replacements\n",
    "        sampled_prompt = (sampled_prompt \n",
    "                          .replace(\"{company name}\", company_name)\n",
    "                          .replace(\"{dominant profile emotion}\", profile_emotion))\n",
    "        reformulation = paraphraseLLM(sampled_prompt)\n",
    "        picture_prompts.append(reformulation)\n",
    "\n",
    "    elif matched_elements.get('picture', [])[1] == 0:\n",
    "        profile_emotion = matched_elements.get('picture', [])[0]\n",
    "        sampled_prompt = random.choice(books['profile_picture_to_change_book'])\n",
    "\n",
    "        if profile_emotion == 'happy':\n",
    "          company_emotion = 'neutral'\n",
    "        else :\n",
    "          company_emotion = 'happy'\n",
    "\n",
    "        # Perform the replacements\n",
    "        sampled_prompt = (sampled_prompt \n",
    "                          .replace(\"{company name}\", company_name)\n",
    "                          .replace(\"{dominant profile emotion}\", profile_emotion)\n",
    "                          .replace(\"{dominant company emotion}\", company_emotion))\n",
    "        reformulation = paraphraseLLM(sampled_prompt)\n",
    "        picture_prompts.append(reformulation)\n",
    "\n",
    "    if matched_elements['theme']:\n",
    "        sampled_prompt = random.choice(books['logo_scheme_book'])\n",
    "        color1, color2 = matched_elements['theme']\n",
    "        sampled_prompt = (sampled_prompt \n",
    "                          .replace(\"{company name}\", company_name)\n",
    "                          .replace(\"{colors}\", color1 + ' and ' + color2))\n",
    "        reformulation = paraphraseLLM(sampled_prompt)\n",
    "        theme_prompts.append(reformulation)\n",
    "\n",
    "    # ALL PROMPTS DICT\n",
    "\n",
    "    instructions = {\n",
    "        'values': values_prompts,\n",
    "        'patterns': patterns_prompts,\n",
    "        'picture': picture_prompts,\n",
    "        'theme': theme_prompts\n",
    "    }\n",
    "\n",
    "    \n",
    "\n",
    "    return instructions\n",
    "\n",
    "\n",
    "\n",
    "from reportlab.lib.pagesizes import A4\n",
    "from reportlab.lib import colors\n",
    "from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, ListFlowable, ListItem\n",
    "from reportlab.lib.styles import getSampleStyleSheet\n",
    "!pip install reportlab\n",
    "def generate_pdf(instructions, profile_id, company_name):\n",
    "    # Define the PDF file name\n",
    "    pdf_filename = f\"{profile_id}_{company_name}.pdf\"\n",
    "    \n",
    "    # Create a PDF document\n",
    "    doc = SimpleDocTemplate(pdf_filename, pagesize=A4)\n",
    "    elements = []\n",
    "    \n",
    "    # Define styles\n",
    "    styles = getSampleStyleSheet()\n",
    "    title_style = styles['Title']\n",
    "    subtitle_style = styles['Heading2']\n",
    "    bullet_style = styles['BodyText']\n",
    "    \n",
    "    # Add Profile ID and Company Name as the main title\n",
    "    elements.append(Paragraph(f\"Profile ID: {profile_id}\", title_style))\n",
    "    elements.append(Paragraph(f\"Company Name: {company_name}\", title_style))\n",
    "    elements.append(Spacer(1, 12))  # Add space after title\n",
    "    \n",
    "    # Function to add a section with a title and bullet points\n",
    "    def add_section(title, items, max_items=3):\n",
    "        if items:\n",
    "            elements.append(Paragraph(title, subtitle_style))\n",
    "            bullet_points = [ListItem(Paragraph(item, bullet_style)) for item in items[:max_items]]\n",
    "            elements.append(ListFlowable(bullet_points, bulletType=\"bullet\"))\n",
    "            elements.append(Spacer(1, 12))  # Add space after section\n",
    "    \n",
    "    # Add sections for VALUES, PATTERNS, PICTURE, THEME\n",
    "    add_section(\"VALUES\", instructions.get('values', []))\n",
    "    add_section(\"PATTERNS\", instructions.get('patterns', []))\n",
    "    add_section(\"PICTURE\", instructions.get('picture', []), max_items=1)\n",
    "    add_section(\"THEME\", instructions.get('theme', []), max_items=1)\n",
    "    \n",
    "    # Build the PDF\n",
    "    doc.build(elements)\n",
    "    \n",
    "    return pdf_filename\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# FINAL USER FUNCTION FOR FRONT END INTERFACE\n",
    "def user_function(profile_id, company_name):\n",
    "    profile_elements = enhance_profiles(profile_id)\n",
    "    company_elements = enhance_companies(company_id)\n",
    "    instructions = generate_instructions(profile_elements, company_elements)\n",
    "    pdf_filename = generate_pdf(instructions, profile_id, company_name)\n",
    "    return pdf_filename\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e5e194d4-08a9-49b6-9f04-f642f7ef342c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# --- TESTING AREA ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1cc34624-15fc-4433-9c7e-7bf1985fb383",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, size\n",
    "\n",
    "# Assuming your DataFrame is called 'df'\n",
    "profiles_filtered = profiles.filter((col(\"about\").isNotNull()) & (size(col(\"recommendations\")) > 1) & (size(col(\"experience\")) > 2) & (size(col(\"volunteer_experience\")) > 1) & (col('top_university') == lit(1)))\n",
    "\n",
    "# Display the filtered DataFrame\n",
    "display(profiles_filtered)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47e2df94-2ff3-40db-a945-3d2bb90e85cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, size\n",
    "\n",
    "# Assuming your DataFrame is called 'df'\n",
    "companies_filtered = companies_values.filter((col(\"about\").isNotNull()) & (col(\"slogan\").isNotNull()) & (col(\"scraped_data\").isNotNull()))\n",
    "\n",
    "# Display the filtered DataFrame\n",
    "display(companies_filtered)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 9006594411633794,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "On_Target",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
